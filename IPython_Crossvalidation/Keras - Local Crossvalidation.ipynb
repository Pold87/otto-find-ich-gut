{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 210\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "'''\n",
    "    This demonstrates how to reach a score of 0.4890 (local validation)\n",
    "    on the Kaggle Otto challenge, with a deep net using Keras.\n",
    "    Compatible Python 2.7-3.4 \n",
    "    Recommended to run on GPU: \n",
    "        Command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kaggle_otto_nn.py\n",
    "        On EC2 g2.2xlarge instance: 19s/epoch. 6-7 minutes total training time.\n",
    "    Best validation score at epoch 21: 0.4881 \n",
    "    Try it at home:\n",
    "        - with/without BatchNormalization (BatchNormalization helps!)\n",
    "        - with ReLU or with PReLU (PReLU helps!)\n",
    "        - with smaller layers, largers layers\n",
    "        - with more layers, less layers\n",
    "        - with different optimizers (SGD+momentum+decay is probably better than Adam!)\n",
    "'''\n",
    "\n",
    "np.random.seed(83415)\n",
    "\n",
    "def load_data(path, train=True):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    if train:\n",
    "        np.random.shuffle(X) # https://youtu.be/uyUXoap67N8\n",
    "        X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "        return X, labels\n",
    "    else:\n",
    "        X, ids = X[:, 1:-1].astype(np.float32), X[:, 0].astype(str)\n",
    "        return X, ids\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "def preprocess_labels(y, encoder=None, categorical=True):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = np_utils.to_categorical(y)\n",
    "    return y, encoder\n",
    "\n",
    "def make_submission(y_prob, ids, encoder, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join([str(i) for i in encoder.classes_]))\n",
    "        f.write('\\n')\n",
    "        for i, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([i] + [str(p) for p in probs.tolist()])\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "9 classes\n",
      "93 dims\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "X, labels = load_data('../data/train80.csv', train=True)\n",
    "X, scaler = preprocess_data(X)\n",
    "y, encoder = preprocess_labels(labels)\n",
    "\n",
    "X_test, ids = load_data('../data/holdout20.csv', train=False)\n",
    "X_test, _ = preprocess_data(X_test, scaler)\n",
    "\n",
    "nb_classes = y.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "dims = X.shape[1]\n",
    "print(dims, 'dims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n",
      "Epoch 0\n",
      "49503/49503 [==============================] - 106s - loss: 2.1003   \n",
      "Epoch 1\n",
      "49503/49503 [==============================] - 107s - loss: 2.0914   \n",
      "Generating submission...\n",
      "12375/12375 [==============================] - 5s     \n",
      "Wrote submission to file keras-otto.csv.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(dims, 240, init='glorot_uniform'))\n",
    "model.add(PReLU((240,)))\n",
    "model.add(BatchNormalization((240,)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(240, 510, init='glorot_uniform'))\n",
    "model.add(PReLU((510,)))\n",
    "model.add(BatchNormalization((510,)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(510, 480, init='glorot_uniform'))\n",
    "model.add(PReLU((480,)))\n",
    "model.add(BatchNormalization((480,)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(480, nb_classes, init='glorot_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "model.fit(X, y, nb_epoch=2, batch_size=16, validation_split=0)\n",
    "\n",
    "print(\"Generating submission...\")\n",
    "\n",
    "proba = model.predict_proba(X_test)\n",
    "make_submission(proba, ids, encoder, fname='keras-otto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540303279719\n"
     ]
    }
   ],
   "source": [
    "sample_sub = \"../submissions/sampleSubmission.csv\"\n",
    "sample_sub_df = pd.read_csv(sample_sub)\n",
    "\n",
    "def normalize(row, epsilon=1e-15):\n",
    "    \n",
    "    row = row / np.sum(row)\n",
    "    row = np.maximum(epsilon, row)\n",
    "    row = np.minimum(1 - epsilon, row)\n",
    "    \n",
    "    return row\n",
    "    \n",
    "def logloss_mc(y_true, y_probs):\n",
    "    \n",
    "    # Normalize probability data frame\n",
    "    y_probs = y_probs.apply(normalize, axis=1)\n",
    "        \n",
    "    log_vals = []\n",
    "        \n",
    "    for i, y in enumerate(y_true):\n",
    "        c = int(y.split(\"_\")[1])\n",
    "        log_vals.append(- np.log(y_probs.iloc[i,c - 1]))\n",
    "        \n",
    "    return np.mean(log_vals)\n",
    "\n",
    "df_holdout = pd.read_csv(\"../data/test25.csv\")\n",
    "y_valid = df_holdout.target\n",
    "sub1 = pd.read_csv(\"../IPython_Submissions/keras/keras-otto-final-aggregated-1.csv\").iloc[:, 1:]\n",
    "sub2 = pd.read_csv(\"../IPython_Submissions/keras/keras-otto-final-aggregated-2.csv\").iloc[:, 1:]\n",
    "\n",
    "sub = sub1 + sub2\n",
    "\n",
    "ll = logloss_mc(y_valid, sub)\n",
    "print(ll)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
