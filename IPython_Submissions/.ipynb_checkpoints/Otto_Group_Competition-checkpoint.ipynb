{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Challenge using nolearn/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook is meant to help you getting started with nolearn and lasagne in order to train a neural net and make a submission to the Otto Group Product Classification Challenge.\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Get the notebook from the Otto Group repository](https://github.com/ottogroup)\n",
    "* [Nolearn repository](https://github.com/dnouri/nolearn)\n",
    "* [Lasagne repository](https://github.com/benanne/Lasagne)\n",
    "* [A nolearn/lasagne tutorial for convolutional nets](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 210\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_data(path, scaler):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(X)\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, X_test, ids, encoder, name='lasagne-otto-final.csv'):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data('../data/test.csv', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = [w.get_value() for w in nn.get_all_params()]\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_weights_from(self.best_weights)\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [('input', InputLayer),\n",
    "           ('dropoutin', DropoutLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout0', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('dropout1', DropoutLayer),\n",
    "           ('dense2', DenseLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net0 = NeuralNet(layers=layers0,\n",
    "                 \n",
    "                 input_shape=(None, num_features),\n",
    "                 dropoutin_p = 0.0002,\n",
    "                 \n",
    "                 dense0_num_units=1024,\n",
    "                 dropout0_p=0.235,\n",
    "                 \n",
    "                 dense1_num_units=512,\n",
    "                 dropout1_p=0.29,\n",
    "                 \n",
    "                 dense2_num_units=256,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,                 \n",
    "                 \n",
    "                 update_learning_rate=theano.shared(float32(0.03)),\n",
    "                 update_momentum=theano.shared(float32(0.92)),\n",
    "\n",
    "                on_epoch_finished=[\n",
    "                    AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
    "                    AdjustVariable('update_momentum', start=0.92, stop=0.98),\n",
    "        ],\n",
    "                 \n",
    "                 eval_size=None,\n",
    "                 verbose=1,\n",
    "                 max_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DropoutLayer      \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 1024)        \tproduces    1024 outputs\n",
      "  DropoutLayer      \t(None, 1024)        \tproduces    1024 outputs\n",
      "  DenseLayer        \t(None, 512)         \tproduces     512 outputs\n",
      "  DropoutLayer      \t(None, 512)         \tproduces     512 outputs\n",
      "  DenseLayer        \t(None, 256)         \tproduces     256 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.722643\u001b[0m  |  \u001b[32m  0.583593\u001b[0m  |     1.238264  |     76.56%  |  17.5s\n",
      "     2  |  \u001b[94m  0.581147\u001b[0m  |  \u001b[32m  0.544184\u001b[0m  |     1.067924  |     77.98%  |  17.6s\n",
      "     3  |  \u001b[94m  0.547523\u001b[0m  |  \u001b[32m  0.528077\u001b[0m  |     1.036824  |     78.67%  |  17.6s\n",
      "     4  |  \u001b[94m  0.524617\u001b[0m  |  \u001b[32m  0.520074\u001b[0m  |     1.008734  |     79.00%  |  17.1s\n",
      "     5  |  \u001b[94m  0.506728\u001b[0m  |  \u001b[32m  0.516519\u001b[0m  |     0.981044  |     79.29%  |  17.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc2b238c320>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc2a8188490>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc2a8188450>,\n",
       "     dense0_num_units=1024, dense1_num_units=512, dense2_num_units=256,\n",
       "     dropout0_p=0.235, dropout1_p=0.29, dropoutin_p=0.0002, eval_size=0.15,\n",
       "     input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dropoutin', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout0', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout1', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense2', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc2a81ab410>,\n",
       "     max_epochs=300, more_params={},\n",
       "     on_epoch_finished=[<__main__.AdjustVariable object at 0x7fc29cef4690>, <__main__.AdjustVariable object at 0x7fc29cef48d0>],\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc2b1f60390>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc2a81ab1b8>,\n",
       "     update_learning_rate=<CudaNdarrayType(float32, scalar)>,\n",
       "     update_momentum=<CudaNdarrayType(float32, scalar)>,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net0.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to file my_neural_net_submission.csv.\n"
     ]
    }
   ],
   "source": [
    "make_submission(net0, X_test, ids, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
