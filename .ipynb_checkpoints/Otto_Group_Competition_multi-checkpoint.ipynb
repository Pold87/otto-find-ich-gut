{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Challenge using nolearn/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook is meant to help you getting started with nolearn and lasagne in order to train a neural net and make a submission to the Otto Group Product Classification Challenge.\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Get the notebook from the Otto Group repository](https://github.com/ottogroup)\n",
    "* [Nolearn repository](https://github.com/dnouri/nolearn)\n",
    "* [Lasagne repository](https://github.com/benanne/Lasagne)\n",
    "* [A nolearn/lasagne tutorial for convolutional nets](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_data(path, scaler):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(X)\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, X_test, ids, encoder, name='my_neural_net_submission.csv'):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data('data/test.csv', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [('input', InputLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net0 = NeuralNet(layers=layers0,\n",
    "                 \n",
    "                 input_shape=(None, num_features),\n",
    "                 dense0_num_units=275,\n",
    "                 dropout_p=0.45,\n",
    "                 dense1_num_units=275,\n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,\n",
    "                 update_learning_rate=0.009,\n",
    "                 update_momentum=0.91,\n",
    "                 \n",
    "                 eval_size=0.2,\n",
    "                 verbose=1,\n",
    "                 max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 275)         \tproduces     275 outputs\n",
      "  DropoutLayer      \t(None, 275)         \tproduces     275 outputs\n",
      "  DenseLayer        \t(None, 275)         \tproduces     275 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.891391\u001b[0m  |  \u001b[32m  0.653195\u001b[0m  |     1.364664  |     75.10%  |  9.4s\n",
      "     2  |  \u001b[94m  0.681409\u001b[0m  |  \u001b[32m  0.612420\u001b[0m  |     1.112650  |     76.39%  |  10.1s\n",
      "     3  |  \u001b[94m  0.640595\u001b[0m  |  \u001b[32m  0.585888\u001b[0m  |     1.093374  |     77.42%  |  9.4s\n",
      "     4  |  \u001b[94m  0.617803\u001b[0m  |  \u001b[32m  0.573120\u001b[0m  |     1.077964  |     77.74%  |  9.7s"
     ]
    }
   ],
   "source": [
    "net0.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 250)         \tproduces     250 outputs\n",
      "  DropoutLayer      \t(None, 250)         \tproduces     250 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.902314\u001b[0m  |  \u001b[32m  0.665007\u001b[0m  |     1.356849  |     74.67%  |  6.7s\n",
      "     2  |  \u001b[94m  0.697292\u001b[0m  |  \u001b[32m  0.621140\u001b[0m  |     1.122600  |     75.96%  |  6.8s\n",
      "     3  |  \u001b[94m  0.659701\u001b[0m  |  \u001b[32m  0.601324\u001b[0m  |     1.097082  |     76.80%  |  6.7s\n",
      "     4  |  \u001b[94m  0.638440\u001b[0m  |  \u001b[32m  0.585768\u001b[0m  |     1.089919  |     77.37%  |  6.6s\n",
      "     5  |  \u001b[94m  0.618017\u001b[0m  |  \u001b[32m  0.578173\u001b[0m  |     1.068914  |     77.52%  |  6.6s\n",
      "     6  |  \u001b[94m  0.602081\u001b[0m  |  \u001b[32m  0.568308\u001b[0m  |     1.059426  |     77.69%  |  6.6s\n",
      "     7  |  \u001b[94m  0.594581\u001b[0m  |  \u001b[32m  0.559873\u001b[0m  |     1.061992  |     77.84%  |  6.6s\n",
      "     8  |  \u001b[94m  0.586299\u001b[0m  |  \u001b[32m  0.555593\u001b[0m  |     1.055267  |     78.07%  |  6.5s\n",
      "     9  |  \u001b[94m  0.578710\u001b[0m  |  \u001b[32m  0.550773\u001b[0m  |     1.050723  |     78.10%  |  6.6s\n",
      "    10  |  \u001b[94m  0.571376\u001b[0m  |  \u001b[32m  0.547075\u001b[0m  |     1.044419  |     78.52%  |  6.6s\n",
      "    11  |  \u001b[94m  0.564234\u001b[0m  |  \u001b[32m  0.544581\u001b[0m  |     1.036088  |     78.48%  |  6.6s\n",
      "    12  |  \u001b[94m  0.560182\u001b[0m  |  \u001b[32m  0.539322\u001b[0m  |     1.038677  |     78.61%  |  6.5s\n",
      "    13  |  \u001b[94m  0.553852\u001b[0m  |  \u001b[32m  0.536610\u001b[0m  |     1.032131  |     78.68%  |  6.5s\n",
      "    14  |  \u001b[94m  0.549799\u001b[0m  |  \u001b[32m  0.535827\u001b[0m  |     1.026076  |     78.59%  |  6.6s\n",
      "    15  |  \u001b[94m  0.546057\u001b[0m  |  \u001b[32m  0.532642\u001b[0m  |     1.025186  |     78.72%  |  6.5s\n",
      "    16  |  \u001b[94m  0.543515\u001b[0m  |  \u001b[32m  0.530343\u001b[0m  |     1.024838  |     78.83%  |  6.4s\n",
      "    17  |  \u001b[94m  0.537814\u001b[0m  |  \u001b[32m  0.527551\u001b[0m  |     1.019455  |     78.85%  |  6.4s\n",
      "    18  |  \u001b[94m  0.534305\u001b[0m  |  \u001b[32m  0.525020\u001b[0m  |     1.017684  |     79.15%  |  6.4s\n",
      "    19  |  \u001b[94m  0.532650\u001b[0m  |  \u001b[32m  0.521345\u001b[0m  |     1.021684  |     79.24%  |  6.4s\n",
      "    20  |  \u001b[94m  0.526020\u001b[0m  |  \u001b[32m  0.519667\u001b[0m  |     1.012225  |     79.16%  |  6.4s\n",
      "    21  |  \u001b[94m  0.524301\u001b[0m  |    0.520105  |     1.008068  |     79.10%  |  6.3s\n",
      "    22  |  \u001b[94m  0.522445\u001b[0m  |  \u001b[32m  0.519248\u001b[0m  |     1.006156  |     79.15%  |  6.3s\n",
      "    23  |  \u001b[94m  0.519603\u001b[0m  |  \u001b[32m  0.515326\u001b[0m  |     1.008300  |     79.33%  |  6.3s\n",
      "    24  |  \u001b[94m  0.516399\u001b[0m  |    0.517870  |     0.997161  |     79.22%  |  6.3s\n",
      "    25  |  \u001b[94m  0.514579\u001b[0m  |  \u001b[32m  0.513905\u001b[0m  |     1.001311  |     79.23%  |  6.3s\n",
      "    26  |  \u001b[94m  0.513556\u001b[0m  |  \u001b[32m  0.513523\u001b[0m  |     1.000065  |     79.22%  |  6.3s\n",
      "    27  |  \u001b[94m  0.508373\u001b[0m  |  \u001b[32m  0.512502\u001b[0m  |     0.991943  |     79.21%  |  6.3s\n",
      "    28  |  \u001b[94m  0.505858\u001b[0m  |  \u001b[32m  0.510524\u001b[0m  |     0.990861  |     79.31%  |  6.3s\n",
      "    29  |  \u001b[94m  0.505194\u001b[0m  |  \u001b[32m  0.510099\u001b[0m  |     0.990383  |     79.40%  |  6.2s\n",
      "    30  |  \u001b[94m  0.502975\u001b[0m  |    0.510873  |     0.984539  |     79.40%  |  6.3s\n",
      "    31  |  \u001b[94m  0.499214\u001b[0m  |  \u001b[32m  0.508368\u001b[0m  |     0.981992  |     79.61%  |  6.2s\n",
      "    32  |    0.499669  |  \u001b[32m  0.505800\u001b[0m  |     0.987878  |     79.59%  |  6.2s\n",
      "    33  |  \u001b[94m  0.497877\u001b[0m  |    0.506006  |     0.983936  |     79.56%  |  6.2s\n",
      "    34  |  \u001b[94m  0.495179\u001b[0m  |  \u001b[32m  0.504399\u001b[0m  |     0.981720  |     79.67%  |  6.2s\n",
      "    35  |  \u001b[94m  0.491448\u001b[0m  |  \u001b[32m  0.502079\u001b[0m  |     0.978826  |     79.74%  |  6.2s\n",
      "    36  |  \u001b[94m  0.487375\u001b[0m  |    0.506459  |     0.962320  |     79.48%  |  6.2s\n",
      "    37  |    0.490251  |    0.503157  |     0.974349  |     79.43%  |  6.2s\n",
      "    38  |  \u001b[94m  0.487126\u001b[0m  |  \u001b[32m  0.501722\u001b[0m  |     0.970910  |     79.69%  |  6.2s\n",
      "    39  |  \u001b[94m  0.483226\u001b[0m  |  \u001b[32m  0.500812\u001b[0m  |     0.964885  |     79.70%  |  6.2s\n",
      "    40  |    0.486274  |    0.502208  |     0.968272  |     79.68%  |  6.1s\n",
      "    41  |    0.483766  |  \u001b[32m  0.499034\u001b[0m  |     0.969406  |     79.78%  |  6.1s\n",
      "    42  |  \u001b[94m  0.480223\u001b[0m  |  \u001b[32m  0.498650\u001b[0m  |     0.963044  |     80.02%  |  6.1s\n",
      "    43  |    0.481237  |    0.499589  |     0.963264  |     79.86%  |  6.1s\n",
      "    44  |  \u001b[94m  0.479898\u001b[0m  |    0.500177  |     0.959455  |     79.75%  |  6.1s\n",
      "    45  |  \u001b[94m  0.474536\u001b[0m  |    0.498943  |     0.951084  |     79.95%  |  6.1s\n",
      "    46  |    0.477304  |    0.499210  |     0.956119  |     79.90%  |  6.1s\n",
      "    47  |    0.475144  |  \u001b[32m  0.497221\u001b[0m  |     0.955599  |     80.06%  |  6.1s\n",
      "    48  |  \u001b[94m  0.470289\u001b[0m  |    0.497580  |     0.945153  |     79.88%  |  6.1s\n",
      "    49  |    0.471745  |  \u001b[32m  0.495759\u001b[0m  |     0.951561  |     80.22%  |  6.1s\n",
      "    50  |  \u001b[94m  0.468310\u001b[0m  |    0.496986  |     0.942299  |     79.93%  |  6.1s\n",
      "    51  |    0.468725  |  \u001b[32m  0.494958\u001b[0m  |     0.947001  |     80.17%  |  6.1s\n",
      "    52  |  \u001b[94m  0.466706\u001b[0m  |    0.497195  |     0.938678  |     79.99%  |  6.1s\n",
      "    53  |  \u001b[94m  0.465430\u001b[0m  |    0.497059  |     0.936367  |     80.01%  |  6.1s\n",
      "    54  |  \u001b[94m  0.464973\u001b[0m  |    0.495906  |     0.937623  |     80.01%  |  6.1s\n",
      "    55  |  \u001b[94m  0.464060\u001b[0m  |  \u001b[32m  0.494098\u001b[0m  |     0.939206  |     80.20%  |  6.0s\n",
      "    56  |  \u001b[94m  0.463549\u001b[0m  |  \u001b[32m  0.493375\u001b[0m  |     0.939546  |     80.22%  |  6.0s\n",
      "    57  |  \u001b[94m  0.461978\u001b[0m  |  \u001b[32m  0.490661\u001b[0m  |     0.941542  |     80.15%  |  6.0s\n",
      "    58  |  \u001b[94m  0.459882\u001b[0m  |    0.493542  |     0.931799  |     80.31%  |  6.0s\n",
      "    59  |  \u001b[94m  0.459648\u001b[0m  |    0.491459  |     0.935272  |     80.33%  |  6.0s\n",
      "    60  |  \u001b[94m  0.458197\u001b[0m  |    0.494983  |     0.925682  |     80.10%  |  6.0s\n",
      "    61  |  \u001b[94m  0.457476\u001b[0m  |    0.490771  |     0.932158  |     80.17%  |  6.0s\n",
      "    62  |    0.458083  |  \u001b[32m  0.489458\u001b[0m  |     0.935900  |     80.39%  |  6.0s\n",
      "    63  |  \u001b[94m  0.454016\u001b[0m  |    0.491072  |     0.924540  |     80.22%  |  6.0s\n",
      "    64  |  \u001b[94m  0.452980\u001b[0m  |  \u001b[32m  0.488269\u001b[0m  |     0.927728  |     80.32%  |  6.0s\n",
      "    65  |    0.453918  |  \u001b[32m  0.487344\u001b[0m  |     0.931413  |     80.47%  |  6.0s\n",
      "    66  |  \u001b[94m  0.450048\u001b[0m  |    0.488863  |     0.920601  |     80.40%  |  6.0s\n",
      "    67  |  \u001b[94m  0.448381\u001b[0m  |    0.488764  |     0.917378  |     80.34%  |  6.0s\n",
      "    68  |    0.450430  |    0.491966  |     0.915572  |     80.55%  |  6.0s\n",
      "    69  |  \u001b[94m  0.447782\u001b[0m  |    0.492933  |     0.908403  |     80.56%  |  6.0s\n",
      "    70  |    0.450461  |    0.489884  |     0.919525  |     80.31%  |  6.0s\n",
      "    71  |    0.448818  |    0.489039  |     0.917755  |     80.26%  |  6.0s\n",
      "    72  |  \u001b[94m  0.447494\u001b[0m  |    0.490339  |     0.912623  |     80.48%  |  6.0s\n",
      "    73  |  \u001b[94m  0.447195\u001b[0m  |    0.488988  |     0.914532  |     80.33%  |  6.0s\n",
      "    74  |  \u001b[94m  0.444793\u001b[0m  |    0.489342  |     0.908960  |     80.41%  |  6.0s\n",
      "    75  |  \u001b[94m  0.444729\u001b[0m  |    0.489287  |     0.908932  |     80.41%  |  6.0s\n",
      "    76  |  \u001b[94m  0.443847\u001b[0m  |    0.488107  |     0.909323  |     80.42%  |  6.0s\n",
      "    77  |  \u001b[94m  0.442797\u001b[0m  |    0.488389  |     0.906647  |     80.64%  |  6.0s\n",
      "    78  |    0.443410  |    0.488444  |     0.907801  |     80.51%  |  6.0s\n",
      "    79  |  \u001b[94m  0.439219\u001b[0m  |    0.490360  |     0.895707  |     80.40%  |  6.0s\n",
      "    80  |  \u001b[94m  0.437389\u001b[0m  |    0.491125  |     0.890587  |     80.64%  |  6.0s\n",
      "    81  |    0.439512  |  \u001b[32m  0.485461\u001b[0m  |     0.905349  |     80.67%  |  6.0s\n",
      "    82  |  \u001b[94m  0.436956\u001b[0m  |    0.487621  |     0.896098  |     80.51%  |  6.0s\n",
      "    83  |  \u001b[94m  0.435782\u001b[0m  |    0.488537  |     0.892014  |     80.47%  |  6.0s\n",
      "    84  |    0.437652  |    0.488216  |     0.896431  |     80.58%  |  6.0s\n",
      "    85  |  \u001b[94m  0.435535\u001b[0m  |    0.487438  |     0.893518  |     80.54%  |  6.0s\n",
      "    86  |  \u001b[94m  0.435101\u001b[0m  |    0.486095  |     0.895094  |     80.43%  |  6.0s\n",
      "    87  |  \u001b[94m  0.431585\u001b[0m  |  \u001b[32m  0.484826\u001b[0m  |     0.890187  |     80.30%  |  6.0s\n",
      "    88  |    0.435397  |    0.487010  |     0.894021  |     80.50%  |  6.0s\n",
      "    89  |  \u001b[94m  0.430754\u001b[0m  |    0.488045  |     0.882612  |     80.46%  |  5.9s\n",
      "    90  |    0.431729  |    0.487506  |     0.885587  |     80.43%  |  6.0s\n",
      "    91  |  \u001b[94m  0.430443\u001b[0m  |    0.489397  |     0.879537  |     80.24%  |  6.0s\n",
      "    92  |    0.431563  |    0.490850  |     0.879216  |     80.31%  |  6.0s\n",
      "    93  |  \u001b[94m  0.426709\u001b[0m  |    0.487891  |     0.874598  |     80.51%  |  6.0s\n",
      "    94  |    0.430076  |    0.486569  |     0.883895  |     80.76%  |  5.9s\n",
      "    95  |    0.429736  |    0.486737  |     0.882892  |     80.56%  |  6.0s\n",
      "    96  |  \u001b[94m  0.423959\u001b[0m  |    0.488694  |     0.867535  |     80.66%  |  5.9s\n",
      "    97  |    0.425169  |    0.486324  |     0.874251  |     80.65%  |  5.9s\n",
      "    98  |    0.425981  |    0.487631  |     0.873573  |     80.58%  |  5.9s\n",
      "    99  |  \u001b[94m  0.423558\u001b[0m  |    0.487201  |     0.869372  |     80.56%  |  5.9s\n",
      "   100  |    0.425415  |    0.488799  |     0.870328  |     80.51%  |  5.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=250, dense1_num_units=200, dropout_p=0.5,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DropoutLayer      \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.899507\u001b[0m  |  \u001b[32m  0.655253\u001b[0m  |     1.372762  |     75.40%  |  10.5s\n",
      "     2  |  \u001b[94m  0.684028\u001b[0m  |  \u001b[32m  0.613025\u001b[0m  |     1.115825  |     76.48%  |  10.4s\n",
      "     3  |  \u001b[94m  0.644799\u001b[0m  |  \u001b[32m  0.591140\u001b[0m  |     1.090771  |     76.90%  |  10.4s\n",
      "     4  |  \u001b[94m  0.624222\u001b[0m  |  \u001b[32m  0.578944\u001b[0m  |     1.078209  |     77.35%  |  10.4s\n",
      "     5  |  \u001b[94m  0.608369\u001b[0m  |  \u001b[32m  0.568021\u001b[0m  |     1.071034  |     77.50%  |  10.4s\n",
      "     6  |  \u001b[94m  0.593068\u001b[0m  |  \u001b[32m  0.558708\u001b[0m  |     1.061498  |     77.86%  |  10.4s\n",
      "     7  |  \u001b[94m  0.579627\u001b[0m  |  \u001b[32m  0.553668\u001b[0m  |     1.046886  |     78.11%  |  10.3s\n",
      "     8  |  \u001b[94m  0.575498\u001b[0m  |  \u001b[32m  0.549413\u001b[0m  |     1.047478  |     78.54%  |  10.3s\n",
      "     9  |  \u001b[94m  0.567629\u001b[0m  |  \u001b[32m  0.543379\u001b[0m  |     1.044628  |     78.34%  |  10.3s\n",
      "    10  |  \u001b[94m  0.558741\u001b[0m  |  \u001b[32m  0.539638\u001b[0m  |     1.035401  |     78.64%  |  10.2s\n",
      "    11  |  \u001b[94m  0.554838\u001b[0m  |  \u001b[32m  0.537051\u001b[0m  |     1.033120  |     78.69%  |  10.2s\n",
      "    12  |  \u001b[94m  0.547981\u001b[0m  |  \u001b[32m  0.532556\u001b[0m  |     1.028965  |     79.08%  |  10.1s\n",
      "    13  |  \u001b[94m  0.541214\u001b[0m  |  \u001b[32m  0.531863\u001b[0m  |     1.017582  |     78.80%  |  10.1s\n",
      "    14  |  \u001b[94m  0.539314\u001b[0m  |  \u001b[32m  0.529890\u001b[0m  |     1.017786  |     78.97%  |  10.1s\n",
      "    15  |  \u001b[94m  0.534739\u001b[0m  |  \u001b[32m  0.527290\u001b[0m  |     1.014125  |     78.97%  |  10.0s\n",
      "    16  |  \u001b[94m  0.529768\u001b[0m  |  \u001b[32m  0.523421\u001b[0m  |     1.012126  |     78.95%  |  10.0s\n",
      "    17  |  \u001b[94m  0.523420\u001b[0m  |  \u001b[32m  0.521945\u001b[0m  |     1.002827  |     79.18%  |  10.0s\n",
      "    18  |  \u001b[94m  0.519827\u001b[0m  |  \u001b[32m  0.520475\u001b[0m  |     0.998757  |     79.21%  |  10.0s\n",
      "    19  |  \u001b[94m  0.519027\u001b[0m  |  \u001b[32m  0.518337\u001b[0m  |     1.001332  |     79.28%  |  9.9s\n",
      "    20  |  \u001b[94m  0.515633\u001b[0m  |    0.518590  |     0.994298  |     79.19%  |  9.9s\n",
      "    21  |  \u001b[94m  0.510506\u001b[0m  |  \u001b[32m  0.513966\u001b[0m  |     0.993267  |     79.50%  |  9.9s\n",
      "    22  |  \u001b[94m  0.508552\u001b[0m  |    0.515474  |     0.986572  |     79.52%  |  9.9s\n",
      "    23  |  \u001b[94m  0.504979\u001b[0m  |  \u001b[32m  0.510605\u001b[0m  |     0.988983  |     79.55%  |  9.9s\n",
      "    24  |  \u001b[94m  0.503184\u001b[0m  |  \u001b[32m  0.507299\u001b[0m  |     0.991888  |     79.45%  |  9.9s\n",
      "    25  |  \u001b[94m  0.500442\u001b[0m  |    0.508484  |     0.984184  |     79.42%  |  9.8s\n",
      "    26  |    0.500957  |  \u001b[32m  0.506829\u001b[0m  |     0.988413  |     79.70%  |  9.8s\n",
      "    27  |  \u001b[94m  0.494168\u001b[0m  |    0.509703  |     0.969521  |     79.64%  |  9.8s\n",
      "    28  |  \u001b[94m  0.492217\u001b[0m  |    0.508482  |     0.968013  |     79.54%  |  9.8s\n",
      "    29  |  \u001b[94m  0.488220\u001b[0m  |  \u001b[32m  0.503900\u001b[0m  |     0.968882  |     79.83%  |  9.8s\n",
      "    30  |    0.489234  |    0.505291  |     0.968223  |     79.80%  |  9.8s\n",
      "    31  |  \u001b[94m  0.484084\u001b[0m  |    0.504675  |     0.959200  |     79.84%  |  9.7s\n",
      "    32  |    0.484121  |  \u001b[32m  0.501244\u001b[0m  |     0.965839  |     79.89%  |  9.7s\n",
      "    33  |  \u001b[94m  0.482365\u001b[0m  |  \u001b[32m  0.501085\u001b[0m  |     0.962641  |     80.02%  |  9.7s\n",
      "    34  |  \u001b[94m  0.478889\u001b[0m  |    0.501397  |     0.955109  |     79.91%  |  9.7s\n",
      "    35  |  \u001b[94m  0.478590\u001b[0m  |  \u001b[32m  0.500796\u001b[0m  |     0.955659  |     80.04%  |  9.7s\n",
      "    36  |  \u001b[94m  0.478007\u001b[0m  |  \u001b[32m  0.499656\u001b[0m  |     0.956672  |     79.90%  |  9.7s\n",
      "    37  |  \u001b[94m  0.472735\u001b[0m  |  \u001b[32m  0.498089\u001b[0m  |     0.949098  |     79.97%  |  9.7s\n",
      "    38  |    0.473191  |    0.499845  |     0.946674  |     79.97%  |  9.7s\n",
      "    39  |  \u001b[94m  0.472379\u001b[0m  |    0.498700  |     0.947220  |     80.02%  |  9.7s\n",
      "    40  |  \u001b[94m  0.468366\u001b[0m  |    0.498588  |     0.939385  |     80.20%  |  9.6s\n",
      "    41  |  \u001b[94m  0.468211\u001b[0m  |  \u001b[32m  0.496500\u001b[0m  |     0.943023  |     80.38%  |  9.6s\n",
      "    42  |  \u001b[94m  0.467279\u001b[0m  |    0.497290  |     0.939651  |     80.13%  |  9.6s\n",
      "    43  |  \u001b[94m  0.466287\u001b[0m  |    0.498433  |     0.935504  |     80.12%  |  9.6s\n",
      "    44  |    0.467192  |    0.497055  |     0.939921  |     80.26%  |  9.6s\n",
      "    45  |  \u001b[94m  0.461846\u001b[0m  |  \u001b[32m  0.494014\u001b[0m  |     0.934883  |     80.15%  |  9.6s\n",
      "    46  |  \u001b[94m  0.460155\u001b[0m  |    0.494421  |     0.930694  |     80.16%  |  9.6s\n",
      "    47  |  \u001b[94m  0.458923\u001b[0m  |  \u001b[32m  0.492394\u001b[0m  |     0.932024  |     80.27%  |  9.6s\n",
      "    48  |    0.459066  |    0.495347  |     0.926757  |     80.17%  |  9.6s\n",
      "    49  |  \u001b[94m  0.458917\u001b[0m  |    0.493020  |     0.930828  |     80.37%  |  9.5s\n",
      "    50  |  \u001b[94m  0.454202\u001b[0m  |    0.492807  |     0.921662  |     80.48%  |  9.5s\n",
      "    51  |  \u001b[94m  0.453201\u001b[0m  |    0.492851  |     0.919549  |     80.39%  |  9.5s\n",
      "    52  |  \u001b[94m  0.452414\u001b[0m  |    0.494540  |     0.914817  |     80.31%  |  9.5s\n",
      "    53  |  \u001b[94m  0.449828\u001b[0m  |    0.495515  |     0.907799  |     80.22%  |  9.5s\n",
      "    54  |    0.452597  |  \u001b[32m  0.491401\u001b[0m  |     0.921032  |     80.42%  |  9.5s\n",
      "    55  |  \u001b[94m  0.447137\u001b[0m  |    0.493214  |     0.906578  |     80.42%  |  9.5s\n",
      "    56  |    0.448904  |    0.493375  |     0.909865  |     80.35%  |  9.5s\n",
      "    57  |  \u001b[94m  0.445268\u001b[0m  |    0.492494  |     0.904108  |     80.58%  |  9.5s\n",
      "    58  |  \u001b[94m  0.444561\u001b[0m  |  \u001b[32m  0.489390\u001b[0m  |     0.908398  |     80.51%  |  9.5s\n",
      "    59  |  \u001b[94m  0.444422\u001b[0m  |    0.491128  |     0.904902  |     80.62%  |  9.4s\n",
      "    60  |  \u001b[94m  0.440303\u001b[0m  |    0.491766  |     0.895350  |     80.61%  |  9.4s\n",
      "    61  |    0.441574  |    0.491211  |     0.898950  |     80.47%  |  9.4s\n",
      "    62  |    0.442507  |  \u001b[32m  0.488229\u001b[0m  |     0.906351  |     80.63%  |  9.4s\n",
      "    63  |    0.440976  |    0.490849  |     0.898394  |     80.42%  |  9.4s\n",
      "    64  |  \u001b[94m  0.437436\u001b[0m  |    0.492755  |     0.887735  |     80.44%  |  9.4s\n",
      "    65  |  \u001b[94m  0.436501\u001b[0m  |  \u001b[32m  0.486461\u001b[0m  |     0.897298  |     80.64%  |  9.4s\n",
      "    66  |  \u001b[94m  0.433861\u001b[0m  |    0.486729  |     0.891381  |     80.53%  |  9.4s\n",
      "    67  |    0.434873  |    0.486898  |     0.893149  |     80.45%  |  9.4s\n",
      "    68  |    0.436097  |  \u001b[32m  0.485264\u001b[0m  |     0.898681  |     80.62%  |  9.4s\n",
      "    69  |    0.435539  |    0.487411  |     0.893577  |     80.43%  |  9.4s\n",
      "    70  |  \u001b[94m  0.430757\u001b[0m  |    0.488332  |     0.882098  |     80.46%  |  9.4s\n",
      "    71  |  \u001b[94m  0.428313\u001b[0m  |    0.490375  |     0.873439  |     80.46%  |  9.4s\n",
      "    72  |  \u001b[94m  0.427163\u001b[0m  |    0.488762  |     0.873969  |     80.48%  |  9.4s\n",
      "    73  |    0.429974  |  \u001b[32m  0.484981\u001b[0m  |     0.886579  |     80.80%  |  9.4s\n",
      "    74  |  \u001b[94m  0.426765\u001b[0m  |    0.489063  |     0.872617  |     80.68%  |  9.3s\n",
      "    75  |  \u001b[94m  0.425028\u001b[0m  |    0.489135  |     0.868939  |     80.71%  |  9.3s\n",
      "    76  |    0.425301  |    0.485862  |     0.875354  |     80.65%  |  9.4s\n",
      "    77  |  \u001b[94m  0.420664\u001b[0m  |    0.485928  |     0.865692  |     80.84%  |  9.4s\n",
      "    78  |    0.422833  |    0.488147  |     0.866201  |     80.96%  |  9.3s\n",
      "    79  |  \u001b[94m  0.419238\u001b[0m  |    0.486464  |     0.861808  |     80.86%  |  9.3s\n",
      "    80  |    0.422753  |    0.487299  |     0.867544  |     80.56%  |  9.3s\n",
      "    81  |    0.422741  |    0.488760  |     0.864925  |     80.65%  |  9.4s\n",
      "    82  |    0.421733  |    0.485253  |     0.869100  |     80.78%  |  9.3s\n",
      "    83  |    0.422418  |    0.485762  |     0.869599  |     80.88%  |  9.3s\n",
      "    84  |  \u001b[94m  0.415816\u001b[0m  |    0.489001  |     0.850337  |     81.02%  |  9.3s\n",
      "    85  |  \u001b[94m  0.414570\u001b[0m  |    0.487605  |     0.850217  |     80.78%  |  9.4s\n",
      "    86  |    0.416297  |    0.485505  |     0.857452  |     80.93%  |  9.3s\n",
      "    87  |    0.415125  |    0.485406  |     0.855211  |     80.83%  |  9.3s\n",
      "    88  |  \u001b[94m  0.410864\u001b[0m  |  \u001b[32m  0.482293\u001b[0m  |     0.851898  |     80.99%  |  9.3s\n",
      "    89  |    0.413660  |    0.484406  |     0.853953  |     81.07%  |  9.3s\n",
      "    90  |    0.413433  |    0.485414  |     0.851712  |     80.70%  |  9.3s\n",
      "    91  |    0.412395  |    0.483533  |     0.852878  |     81.00%  |  9.3s\n",
      "    92  |    0.410912  |    0.486871  |     0.843986  |     80.75%  |  9.3s\n",
      "    93  |  \u001b[94m  0.409777\u001b[0m  |    0.484136  |     0.846409  |     80.69%  |  9.3s\n",
      "    94  |  \u001b[94m  0.408766\u001b[0m  |    0.484519  |     0.843655  |     80.70%  |  9.3s\n",
      "    95  |  \u001b[94m  0.407283\u001b[0m  |    0.487578  |     0.835319  |     80.88%  |  9.3s\n",
      "    96  |    0.408561  |    0.486746  |     0.839373  |     80.97%  |  9.3s\n",
      "    97  |  \u001b[94m  0.404832\u001b[0m  |    0.485414  |     0.833993  |     80.86%  |  9.3s\n",
      "    98  |    0.405599  |    0.484454  |     0.837230  |     80.93%  |  9.3s\n",
      "    99  |  \u001b[94m  0.404294\u001b[0m  |    0.487848  |     0.828731  |     80.75%  |  9.3s\n",
      "   100  |    0.405579  |    0.486811  |     0.833135  |     80.76%  |  9.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=300, dense1_num_units=300, dropout_p=0.5,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DropoutLayer      \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  1.056229\u001b[0m  |  \u001b[32m  0.699174\u001b[0m  |     1.510681  |     73.86%  |  4.7s\n",
      "     2  |  \u001b[94m  0.796770\u001b[0m  |  \u001b[32m  0.658425\u001b[0m  |     1.210116  |     74.60%  |  4.7s\n",
      "     3  |  \u001b[94m  0.747434\u001b[0m  |  \u001b[32m  0.636940\u001b[0m  |     1.173476  |     75.16%  |  4.7s\n",
      "     4  |  \u001b[94m  0.721201\u001b[0m  |  \u001b[32m  0.625434\u001b[0m  |     1.153120  |     75.24%  |  4.7s\n",
      "     5  |  \u001b[94m  0.698607\u001b[0m  |  \u001b[32m  0.614030\u001b[0m  |     1.137739  |     75.99%  |  4.7s\n",
      "     6  |  \u001b[94m  0.686315\u001b[0m  |  \u001b[32m  0.605392\u001b[0m  |     1.133669  |     76.03%  |  4.7s\n",
      "     7  |  \u001b[94m  0.674825\u001b[0m  |  \u001b[32m  0.598250\u001b[0m  |     1.127998  |     76.27%  |  4.7s\n",
      "     8  |  \u001b[94m  0.666965\u001b[0m  |  \u001b[32m  0.596289\u001b[0m  |     1.118526  |     76.58%  |  4.7s\n",
      "     9  |  \u001b[94m  0.660558\u001b[0m  |  \u001b[32m  0.590155\u001b[0m  |     1.119296  |     76.67%  |  4.6s\n",
      "    10  |  \u001b[94m  0.655216\u001b[0m  |  \u001b[32m  0.585114\u001b[0m  |     1.119808  |     76.91%  |  4.6s\n",
      "    11  |  \u001b[94m  0.646966\u001b[0m  |  \u001b[32m  0.583831\u001b[0m  |     1.108139  |     76.94%  |  4.6s\n",
      "    12  |  \u001b[94m  0.641469\u001b[0m  |  \u001b[32m  0.580021\u001b[0m  |     1.105941  |     76.93%  |  4.6s\n",
      "    13  |  \u001b[94m  0.638001\u001b[0m  |  \u001b[32m  0.577687\u001b[0m  |     1.104405  |     76.95%  |  4.6s\n",
      "    14  |  \u001b[94m  0.633280\u001b[0m  |  \u001b[32m  0.576160\u001b[0m  |     1.099139  |     77.21%  |  4.6s\n",
      "    15  |  \u001b[94m  0.628583\u001b[0m  |  \u001b[32m  0.573362\u001b[0m  |     1.096312  |     77.18%  |  4.6s\n",
      "    16  |  \u001b[94m  0.628427\u001b[0m  |  \u001b[32m  0.569940\u001b[0m  |     1.102619  |     77.31%  |  4.6s\n",
      "    17  |  \u001b[94m  0.621922\u001b[0m  |  \u001b[32m  0.568686\u001b[0m  |     1.093611  |     77.34%  |  4.6s\n",
      "    18  |    0.623102  |  \u001b[32m  0.567417\u001b[0m  |     1.098138  |     77.37%  |  4.6s\n",
      "    19  |  \u001b[94m  0.618903\u001b[0m  |  \u001b[32m  0.563796\u001b[0m  |     1.097742  |     77.50%  |  4.6s\n",
      "    20  |  \u001b[94m  0.614524\u001b[0m  |    0.567257  |     1.083325  |     77.26%  |  4.6s\n",
      "    21  |  \u001b[94m  0.611387\u001b[0m  |    0.564403  |     1.083244  |     77.65%  |  4.6s\n",
      "    22  |  \u001b[94m  0.608584\u001b[0m  |  \u001b[32m  0.561860\u001b[0m  |     1.083160  |     77.60%  |  4.6s\n",
      "    23  |  \u001b[94m  0.604557\u001b[0m  |  \u001b[32m  0.560062\u001b[0m  |     1.079447  |     77.68%  |  4.6s\n",
      "    24  |    0.607577  |    0.562438  |     1.080256  |     77.45%  |  4.6s\n",
      "    25  |  \u001b[94m  0.602822\u001b[0m  |  \u001b[32m  0.559290\u001b[0m  |     1.077834  |     77.65%  |  4.6s\n",
      "    26  |  \u001b[94m  0.601611\u001b[0m  |    0.562505  |     1.069521  |     77.46%  |  4.6s\n",
      "    27  |    0.601808  |  \u001b[32m  0.558770\u001b[0m  |     1.077024  |     77.80%  |  4.6s\n",
      "    28  |  \u001b[94m  0.597257\u001b[0m  |  \u001b[32m  0.556149\u001b[0m  |     1.073915  |     77.97%  |  4.6s\n",
      "    29  |  \u001b[94m  0.595330\u001b[0m  |    0.557440  |     1.067971  |     77.70%  |  4.5s\n",
      "    30  |  \u001b[94m  0.592979\u001b[0m  |  \u001b[32m  0.552176\u001b[0m  |     1.073896  |     78.04%  |  4.5s\n",
      "    31  |  \u001b[94m  0.592692\u001b[0m  |    0.552763  |     1.072235  |     77.99%  |  4.5s\n",
      "    32  |  \u001b[94m  0.588945\u001b[0m  |  \u001b[32m  0.550807\u001b[0m  |     1.069241  |     78.05%  |  4.5s\n",
      "    33  |    0.590930  |  \u001b[32m  0.550774\u001b[0m  |     1.072909  |     77.98%  |  4.5s\n",
      "    34  |  \u001b[94m  0.586069\u001b[0m  |    0.553259  |     1.059303  |     78.05%  |  4.5s\n",
      "    35  |    0.588387  |  \u001b[32m  0.550589\u001b[0m  |     1.068650  |     77.96%  |  4.5s\n",
      "    36  |  \u001b[94m  0.585181\u001b[0m  |  \u001b[32m  0.547184\u001b[0m  |     1.069441  |     78.18%  |  4.5s\n",
      "    37  |  \u001b[94m  0.584279\u001b[0m  |    0.548466  |     1.065297  |     78.33%  |  4.5s\n",
      "    38  |  \u001b[94m  0.583310\u001b[0m  |  \u001b[32m  0.545216\u001b[0m  |     1.069870  |     78.11%  |  4.5s\n",
      "    39  |    0.586352  |    0.546326  |     1.073265  |     78.29%  |  4.5s\n",
      "    40  |  \u001b[94m  0.579224\u001b[0m  |    0.546136  |     1.060584  |     78.39%  |  4.5s\n",
      "    41  |    0.582639  |  \u001b[32m  0.543830\u001b[0m  |     1.071363  |     78.33%  |  4.5s\n",
      "    42  |  \u001b[94m  0.578714\u001b[0m  |    0.546705  |     1.058550  |     78.22%  |  4.5s\n",
      "    43  |  \u001b[94m  0.578310\u001b[0m  |    0.544714  |     1.061677  |     78.39%  |  4.5s\n",
      "    44  |  \u001b[94m  0.575141\u001b[0m  |  \u001b[32m  0.542757\u001b[0m  |     1.059665  |     78.30%  |  4.5s\n",
      "    45  |    0.578040  |  \u001b[32m  0.541557\u001b[0m  |     1.067367  |     78.47%  |  4.5s\n",
      "    46  |  \u001b[94m  0.574832\u001b[0m  |    0.541957  |     1.060659  |     78.30%  |  4.5s\n",
      "    47  |  \u001b[94m  0.574281\u001b[0m  |    0.543636  |     1.056369  |     78.35%  |  4.5s\n",
      "    48  |  \u001b[94m  0.571217\u001b[0m  |    0.543382  |     1.051226  |     78.14%  |  4.5s\n",
      "    49  |    0.572738  |  \u001b[32m  0.540459\u001b[0m  |     1.059724  |     78.50%  |  4.5s\n",
      "    50  |    0.573349  |  \u001b[32m  0.539099\u001b[0m  |     1.063532  |     78.42%  |  4.5s\n",
      "    51  |  \u001b[94m  0.571016\u001b[0m  |    0.539964  |     1.057509  |     78.51%  |  4.5s\n",
      "    52  |    0.571278  |    0.542088  |     1.053847  |     78.39%  |  4.5s\n",
      "    53  |  \u001b[94m  0.569394\u001b[0m  |  \u001b[32m  0.537727\u001b[0m  |     1.058891  |     78.65%  |  4.5s\n",
      "    54  |    0.570304  |    0.539717  |     1.056672  |     78.57%  |  4.5s\n",
      "    55  |  \u001b[94m  0.568247\u001b[0m  |    0.537838  |     1.056539  |     78.47%  |  4.5s\n",
      "    56  |    0.569424  |  \u001b[32m  0.536265\u001b[0m  |     1.061833  |     78.70%  |  4.5s\n",
      "    57  |  \u001b[94m  0.565565\u001b[0m  |    0.538346  |     1.050561  |     78.76%  |  4.5s\n",
      "    58  |    0.567857  |  \u001b[32m  0.535620\u001b[0m  |     1.060186  |     78.68%  |  4.5s\n",
      "    59  |  \u001b[94m  0.564225\u001b[0m  |    0.538844  |     1.047102  |     78.64%  |  4.5s\n",
      "    60  |    0.566471  |  \u001b[32m  0.534110\u001b[0m  |     1.060590  |     78.83%  |  4.5s\n",
      "    61  |  \u001b[94m  0.563366\u001b[0m  |    0.534340  |     1.054320  |     78.82%  |  4.5s\n",
      "    62  |    0.564699  |    0.535694  |     1.054144  |     78.68%  |  4.5s\n",
      "    63  |  \u001b[94m  0.562894\u001b[0m  |  \u001b[32m  0.532104\u001b[0m  |     1.057864  |     78.74%  |  4.5s\n",
      "    64  |  \u001b[94m  0.562021\u001b[0m  |    0.534040  |     1.052395  |     78.60%  |  4.5s\n",
      "    65  |  \u001b[94m  0.560222\u001b[0m  |  \u001b[32m  0.531905\u001b[0m  |     1.053238  |     78.92%  |  4.5s\n",
      "    66  |    0.560930  |    0.533736  |     1.050950  |     78.68%  |  4.5s\n",
      "    67  |  \u001b[94m  0.558292\u001b[0m  |    0.533528  |     1.046416  |     78.81%  |  4.5s\n",
      "    68  |    0.560456  |    0.533488  |     1.050550  |     78.92%  |  4.5s\n",
      "    69  |    0.559549  |    0.536559  |     1.042847  |     78.64%  |  4.5s\n",
      "    70  |  \u001b[94m  0.558163\u001b[0m  |    0.534402  |     1.044464  |     78.75%  |  4.5s\n",
      "    71  |    0.558709  |    0.534576  |     1.045144  |     78.85%  |  4.5s\n",
      "    72  |  \u001b[94m  0.555119\u001b[0m  |    0.532505  |     1.042467  |     78.97%  |  4.5s\n",
      "    73  |    0.557053  |    0.537536  |     1.036308  |     78.73%  |  4.5s\n",
      "    74  |    0.558457  |  \u001b[32m  0.531098\u001b[0m  |     1.051515  |     78.93%  |  4.5s\n",
      "    75  |    0.557111  |    0.533541  |     1.044176  |     78.80%  |  4.5s\n",
      "    76  |    0.558106  |    0.531492  |     1.050075  |     78.85%  |  4.5s\n",
      "    77  |  \u001b[94m  0.555028\u001b[0m  |  \u001b[32m  0.529327\u001b[0m  |     1.048553  |     78.68%  |  4.5s\n",
      "    78  |    0.555238  |    0.529879  |     1.047858  |     78.92%  |  4.5s\n",
      "    79  |  \u001b[94m  0.553216\u001b[0m  |  \u001b[32m  0.526669\u001b[0m  |     1.050406  |     79.10%  |  4.3s\n",
      "    80  |  \u001b[94m  0.551257\u001b[0m  |    0.530689  |     1.038758  |     79.05%  |  4.2s\n",
      "    81  |    0.551314  |    0.527206  |     1.045728  |     78.86%  |  4.2s\n",
      "    82  |    0.554743  |    0.529125  |     1.048415  |     79.02%  |  4.2s\n",
      "    83  |    0.554362  |    0.529288  |     1.047372  |     79.06%  |  4.2s\n",
      "    84  |  \u001b[94m  0.550453\u001b[0m  |    0.531210  |     1.036225  |     78.87%  |  4.2s\n",
      "    85  |  \u001b[94m  0.549512\u001b[0m  |    0.530030  |     1.036756  |     78.91%  |  4.2s\n",
      "    86  |    0.551684  |    0.528446  |     1.043973  |     79.03%  |  4.2s\n",
      "    87  |    0.550539  |    0.529002  |     1.040713  |     78.87%  |  4.2s\n",
      "    88  |  \u001b[94m  0.549246\u001b[0m  |    0.529660  |     1.036978  |     78.97%  |  4.2s\n",
      "    89  |  \u001b[94m  0.547799\u001b[0m  |    0.528230  |     1.037047  |     78.89%  |  4.3s\n",
      "    90  |    0.550325  |    0.530858  |     1.036672  |     78.84%  |  4.5s\n",
      "    91  |    0.550045  |    0.528681  |     1.040411  |     78.76%  |  4.5s\n",
      "    92  |    0.549043  |  \u001b[32m  0.525602\u001b[0m  |     1.044598  |     79.01%  |  4.5s\n",
      "    93  |    0.549079  |    0.525660  |     1.044552  |     78.82%  |  4.5s\n",
      "    94  |  \u001b[94m  0.547463\u001b[0m  |    0.527388  |     1.038066  |     78.97%  |  4.5s\n",
      "    95  |  \u001b[94m  0.547416\u001b[0m  |  \u001b[32m  0.524685\u001b[0m  |     1.043323  |     78.99%  |  4.5s\n",
      "    96  |    0.547647  |  \u001b[32m  0.523823\u001b[0m  |     1.045481  |     79.10%  |  4.5s\n",
      "    97  |  \u001b[94m  0.546529\u001b[0m  |    0.523927  |     1.043139  |     78.97%  |  4.5s\n",
      "    98  |    0.548631  |    0.525883  |     1.043257  |     78.96%  |  4.5s\n",
      "    99  |    0.546645  |    0.526850  |     1.037574  |     78.93%  |  4.5s\n",
      "   100  |  \u001b[94m  0.546042\u001b[0m  |    0.526983  |     1.036166  |     79.02%  |  4.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=200, dense1_num_units=200, dropout_p=0.75,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DropoutLayer      \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 200)         \tproduces     200 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  1.056035\u001b[0m  |  \u001b[32m  0.702061\u001b[0m  |     1.504192  |     73.94%  |  4.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=200, dense1_num_units=200, dropout_p=0.75,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.02, update_momentum=0.8,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net4.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 400)         \tproduces     400 outputs\n",
      "  DropoutLayer      \t(None, 400)         \tproduces     400 outputs\n",
      "  DenseLayer        \t(None, 400)         \tproduces     400 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.873218\u001b[0m  |  \u001b[32m  0.655700\u001b[0m  |     1.331735  |     75.26%  |  17.1s\n",
      "     2  |  \u001b[94m  0.671351\u001b[0m  |  \u001b[32m  0.609322\u001b[0m  |     1.101799  |     76.61%  |  17.0s\n",
      "     3  |  \u001b[94m  0.635311\u001b[0m  |  \u001b[32m  0.588956\u001b[0m  |     1.078707  |     77.36%  |  17.2s\n",
      "     4  |  \u001b[94m  0.615298\u001b[0m  |  \u001b[32m  0.577839\u001b[0m  |     1.064826  |     77.39%  |  17.1s\n",
      "     5  |  \u001b[94m  0.594160\u001b[0m  |  \u001b[32m  0.563531\u001b[0m  |     1.054353  |     77.75%  |  16.9s\n",
      "     6  |  \u001b[94m  0.583709\u001b[0m  |  \u001b[32m  0.557675\u001b[0m  |     1.046684  |     77.93%  |  16.9s\n",
      "     7  |  \u001b[94m  0.570840\u001b[0m  |  \u001b[32m  0.552108\u001b[0m  |     1.033927  |     78.40%  |  16.8s\n",
      "     8  |  \u001b[94m  0.564005\u001b[0m  |  \u001b[32m  0.547829\u001b[0m  |     1.029528  |     78.35%  |  16.8s\n",
      "     9  |  \u001b[94m  0.555604\u001b[0m  |  \u001b[32m  0.541089\u001b[0m  |     1.026827  |     78.56%  |  16.7s\n",
      "    10  |  \u001b[94m  0.547006\u001b[0m  |  \u001b[32m  0.539497\u001b[0m  |     1.013918  |     78.57%  |  16.8s\n",
      "    11  |  \u001b[94m  0.538952\u001b[0m  |  \u001b[32m  0.534430\u001b[0m  |     1.008461  |     78.74%  |  17.1s\n",
      "    12  |  \u001b[94m  0.535242\u001b[0m  |    0.535059  |     1.000341  |     78.75%  |  16.6s\n",
      "    13  |  \u001b[94m  0.530788\u001b[0m  |  \u001b[32m  0.530913\u001b[0m  |     0.999765  |     78.78%  |  16.5s\n",
      "    14  |  \u001b[94m  0.526216\u001b[0m  |  \u001b[32m  0.526988\u001b[0m  |     0.998535  |     79.10%  |  16.5s\n",
      "    15  |  \u001b[94m  0.520939\u001b[0m  |  \u001b[32m  0.523566\u001b[0m  |     0.994984  |     79.31%  |  16.4s\n",
      "    16  |  \u001b[94m  0.517628\u001b[0m  |  \u001b[32m  0.520440\u001b[0m  |     0.994596  |     79.10%  |  16.4s\n",
      "    17  |  \u001b[94m  0.512178\u001b[0m  |  \u001b[32m  0.520225\u001b[0m  |     0.984533  |     79.17%  |  16.3s\n",
      "    18  |  \u001b[94m  0.509349\u001b[0m  |  \u001b[32m  0.517947\u001b[0m  |     0.983400  |     79.34%  |  16.3s\n",
      "    19  |  \u001b[94m  0.506882\u001b[0m  |  \u001b[32m  0.514541\u001b[0m  |     0.985115  |     79.28%  |  16.3s\n",
      "    20  |  \u001b[94m  0.501253\u001b[0m  |  \u001b[32m  0.513324\u001b[0m  |     0.976485  |     79.36%  |  16.2s\n",
      "    21  |  \u001b[94m  0.500381\u001b[0m  |    0.514049  |     0.973412  |     79.32%  |  16.1s\n",
      "    22  |  \u001b[94m  0.497787\u001b[0m  |    0.513771  |     0.968890  |     79.22%  |  16.1s\n",
      "    23  |  \u001b[94m  0.492521\u001b[0m  |  \u001b[32m  0.513200\u001b[0m  |     0.959706  |     79.38%  |  16.1s\n",
      "    24  |  \u001b[94m  0.489018\u001b[0m  |    0.514054  |     0.951295  |     79.55%  |  16.1s\n",
      "    25  |  \u001b[94m  0.485943\u001b[0m  |  \u001b[32m  0.509827\u001b[0m  |     0.953152  |     79.40%  |  16.0s\n",
      "    26  |  \u001b[94m  0.484417\u001b[0m  |    0.510478  |     0.948949  |     79.25%  |  16.0s\n",
      "    27  |  \u001b[94m  0.482244\u001b[0m  |  \u001b[32m  0.506866\u001b[0m  |     0.951422  |     79.47%  |  16.0s\n",
      "    28  |  \u001b[94m  0.478267\u001b[0m  |    0.508707  |     0.940162  |     79.42%  |  15.9s\n",
      "    29  |  \u001b[94m  0.474230\u001b[0m  |  \u001b[32m  0.505237\u001b[0m  |     0.938629  |     79.82%  |  16.0s\n",
      "    30  |  \u001b[94m  0.473523\u001b[0m  |  \u001b[32m  0.500922\u001b[0m  |     0.945303  |     79.88%  |  15.9s\n",
      "    31  |  \u001b[94m  0.471533\u001b[0m  |    0.502985  |     0.937470  |     79.84%  |  15.8s\n",
      "    32  |  \u001b[94m  0.468330\u001b[0m  |    0.501543  |     0.933777  |     80.04%  |  15.8s\n",
      "    33  |  \u001b[94m  0.466510\u001b[0m  |    0.502431  |     0.928505  |     79.90%  |  15.8s\n",
      "    34  |  \u001b[94m  0.464334\u001b[0m  |    0.503759  |     0.921738  |     79.88%  |  15.8s\n",
      "    35  |  \u001b[94m  0.462142\u001b[0m  |    0.501589  |     0.921357  |     80.01%  |  15.7s\n",
      "    36  |  \u001b[94m  0.460502\u001b[0m  |  \u001b[32m  0.498115\u001b[0m  |     0.924489  |     80.12%  |  15.7s\n",
      "    37  |  \u001b[94m  0.459219\u001b[0m  |    0.500430  |     0.917648  |     79.87%  |  15.7s\n",
      "    38  |  \u001b[94m  0.456818\u001b[0m  |    0.500407  |     0.912894  |     80.03%  |  15.7s\n",
      "    39  |  \u001b[94m  0.455304\u001b[0m  |    0.503000  |     0.905176  |     79.81%  |  15.7s\n",
      "    40  |  \u001b[94m  0.449691\u001b[0m  |    0.498370  |     0.902322  |     80.01%  |  15.7s\n",
      "    41  |    0.449833  |  \u001b[32m  0.494606\u001b[0m  |     0.909479  |     80.22%  |  15.6s\n",
      "    42  |  \u001b[94m  0.449314\u001b[0m  |    0.495886  |     0.906085  |     80.27%  |  15.6s\n",
      "    43  |  \u001b[94m  0.447801\u001b[0m  |    0.498293  |     0.898671  |     80.11%  |  15.6s\n",
      "    44  |    0.448827  |    0.495800  |     0.905259  |     80.21%  |  15.6s\n",
      "    45  |  \u001b[94m  0.440863\u001b[0m  |  \u001b[32m  0.494045\u001b[0m  |     0.892355  |     80.21%  |  15.6s\n",
      "    46  |    0.442478  |    0.494579  |     0.894656  |     80.24%  |  15.6s\n",
      "    47  |  \u001b[94m  0.440654\u001b[0m  |    0.494172  |     0.891702  |     80.48%  |  15.6s\n",
      "    48  |  \u001b[94m  0.440601\u001b[0m  |    0.494571  |     0.890876  |     80.57%  |  15.6s\n",
      "    49  |  \u001b[94m  0.440476\u001b[0m  |    0.494358  |     0.891005  |     80.41%  |  15.5s\n",
      "    50  |  \u001b[94m  0.435503\u001b[0m  |    0.499793  |     0.871366  |     80.22%  |  15.5s\n",
      "    51  |  \u001b[94m  0.433040\u001b[0m  |    0.495835  |     0.873355  |     80.53%  |  15.5s\n",
      "    52  |  \u001b[94m  0.431294\u001b[0m  |    0.496011  |     0.869525  |     80.23%  |  15.5s\n",
      "    53  |    0.432630  |    0.496474  |     0.871404  |     80.49%  |  15.4s\n",
      "    54  |  \u001b[94m  0.427603\u001b[0m  |    0.496374  |     0.861453  |     80.63%  |  15.4s\n",
      "    55  |    0.429055  |    0.495770  |     0.865431  |     80.49%  |  15.4s\n",
      "    56  |  \u001b[94m  0.427197\u001b[0m  |    0.495323  |     0.862463  |     80.33%  |  15.4s\n",
      "    57  |  \u001b[94m  0.426392\u001b[0m  |  \u001b[32m  0.491596\u001b[0m  |     0.867363  |     80.62%  |  15.4s\n",
      "    58  |  \u001b[94m  0.423642\u001b[0m  |    0.492232  |     0.860655  |     80.60%  |  15.3s\n",
      "    59  |  \u001b[94m  0.421646\u001b[0m  |  \u001b[32m  0.491037\u001b[0m  |     0.858684  |     80.74%  |  15.4s\n",
      "    60  |  \u001b[94m  0.420151\u001b[0m  |  \u001b[32m  0.489848\u001b[0m  |     0.857716  |     80.68%  |  15.3s\n",
      "    61  |  \u001b[94m  0.419631\u001b[0m  |    0.495821  |     0.846335  |     80.49%  |  15.3s\n",
      "    62  |  \u001b[94m  0.419224\u001b[0m  |  \u001b[32m  0.489833\u001b[0m  |     0.855850  |     80.67%  |  15.3s\n",
      "    63  |  \u001b[94m  0.417147\u001b[0m  |    0.492573  |     0.846874  |     80.65%  |  15.2s\n",
      "    64  |  \u001b[94m  0.414525\u001b[0m  |    0.493635  |     0.839739  |     80.70%  |  15.2s\n",
      "    65  |    0.415945  |    0.491160  |     0.846863  |     80.78%  |  15.2s\n",
      "    66  |  \u001b[94m  0.410528\u001b[0m  |    0.490622  |     0.836750  |     80.89%  |  15.2s\n",
      "    67  |    0.412421  |    0.493069  |     0.836437  |     80.73%  |  15.3s\n",
      "    68  |    0.411694  |    0.493972  |     0.833434  |     80.86%  |  15.4s\n",
      "    69  |  \u001b[94m  0.408611\u001b[0m  |    0.490112  |     0.833711  |     80.81%  |  15.3s\n",
      "    70  |  \u001b[94m  0.407094\u001b[0m  |    0.493796  |     0.824419  |     80.87%  |  15.2s\n",
      "    71  |    0.407194  |    0.492912  |     0.826099  |     80.73%  |  15.2s\n",
      "    72  |  \u001b[94m  0.403427\u001b[0m  |    0.492601  |     0.818974  |     80.68%  |  15.3s\n",
      "    73  |    0.406113  |    0.493936  |     0.822199  |     80.62%  |  15.2s\n",
      "    74  |    0.403520  |    0.496370  |     0.812943  |     80.50%  |  15.2s\n",
      "    75  |  \u001b[94m  0.401788\u001b[0m  |    0.493891  |     0.813515  |     80.69%  |  15.2s\n",
      "    76  |  \u001b[94m  0.401655\u001b[0m  |    0.493219  |     0.814355  |     80.72%  |  15.1s\n",
      "    77  |  \u001b[94m  0.400246\u001b[0m  |    0.495019  |     0.808545  |     80.59%  |  15.1s\n",
      "    78  |  \u001b[94m  0.400013\u001b[0m  |    0.491856  |     0.813272  |     80.55%  |  15.1s\n",
      "    79  |  \u001b[94m  0.397338\u001b[0m  |    0.495806  |     0.801399  |     80.64%  |  15.1s\n",
      "    80  |  \u001b[94m  0.394886\u001b[0m  |    0.493044  |     0.800915  |     80.78%  |  15.1s\n",
      "    81  |    0.395323  |    0.494224  |     0.799886  |     80.90%  |  15.1s\n",
      "    82  |    0.395237  |    0.495003  |     0.798454  |     80.57%  |  15.1s\n",
      "    83  |  \u001b[94m  0.391865\u001b[0m  |    0.492627  |     0.795459  |     80.82%  |  15.0s\n",
      "    84  |    0.394270  |    0.490708  |     0.803471  |     80.58%  |  15.0s\n",
      "    85  |  \u001b[94m  0.386556\u001b[0m  |    0.494382  |     0.781897  |     80.61%  |  15.0s\n",
      "    86  |    0.389386  |    0.498009  |     0.781885  |     80.69%  |  15.0s\n",
      "    87  |    0.387998  |    0.495900  |     0.782413  |     80.70%  |  15.0s\n",
      "    88  |    0.386952  |    0.493793  |     0.783632  |     80.77%  |  15.0s\n",
      "    89  |  \u001b[94m  0.385996\u001b[0m  |    0.493608  |     0.781988  |     80.72%  |  15.0s\n",
      "    90  |    0.386032  |    0.493875  |     0.781638  |     80.72%  |  15.0s\n",
      "    91  |    0.386563  |    0.494584  |     0.781592  |     80.77%  |  15.0s\n",
      "    92  |  \u001b[94m  0.382190\u001b[0m  |    0.494173  |     0.773392  |     80.83%  |  15.0s\n",
      "    93  |    0.382753  |    0.494582  |     0.773891  |     80.88%  |  15.0s\n",
      "    94  |  \u001b[94m  0.381599\u001b[0m  |    0.497334  |     0.767290  |     80.65%  |  15.0s\n",
      "    95  |  \u001b[94m  0.379997\u001b[0m  |    0.494567  |     0.768343  |     80.83%  |  15.0s\n",
      "    96  |  \u001b[94m  0.379896\u001b[0m  |    0.496251  |     0.765533  |     80.74%  |  15.0s\n",
      "    97  |    0.380508  |    0.499563  |     0.761681  |     80.82%  |  15.0s\n",
      "    98  |  \u001b[94m  0.379627\u001b[0m  |    0.500684  |     0.758217  |     80.85%  |  14.9s\n",
      "    99  |  \u001b[94m  0.378141\u001b[0m  |    0.494229  |     0.765113  |     80.85%  |  14.9s\n",
      "   100  |  \u001b[94m  0.376621\u001b[0m  |    0.498248  |     0.755890  |     80.82%  |  14.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=400, dense1_num_units=400, dropout_p=0.5,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net5.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DropoutLayer      \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.946256\u001b[0m  |  \u001b[32m  0.674697\u001b[0m  |     1.402490  |     74.25%  |  9.6s\n",
      "     2  |  \u001b[94m  0.727588\u001b[0m  |  \u001b[32m  0.636392\u001b[0m  |     1.143300  |     75.74%  |  9.5s\n",
      "     3  |  \u001b[94m  0.684885\u001b[0m  |  \u001b[32m  0.612714\u001b[0m  |     1.117788  |     76.27%  |  9.6s\n",
      "     4  |  \u001b[94m  0.658795\u001b[0m  |  \u001b[32m  0.596843\u001b[0m  |     1.103800  |     76.74%  |  9.5s\n",
      "     5  |  \u001b[94m  0.644275\u001b[0m  |  \u001b[32m  0.586196\u001b[0m  |     1.099078  |     76.95%  |  9.5s\n",
      "     6  |  \u001b[94m  0.632103\u001b[0m  |  \u001b[32m  0.581170\u001b[0m  |     1.087640  |     77.06%  |  9.5s\n",
      "     7  |  \u001b[94m  0.616982\u001b[0m  |  \u001b[32m  0.571750\u001b[0m  |     1.079110  |     77.34%  |  9.6s\n",
      "     8  |  \u001b[94m  0.609404\u001b[0m  |  \u001b[32m  0.566975\u001b[0m  |     1.074834  |     77.47%  |  9.5s\n",
      "     9  |  \u001b[94m  0.605089\u001b[0m  |  \u001b[32m  0.562057\u001b[0m  |     1.076561  |     77.54%  |  9.4s\n",
      "    10  |  \u001b[94m  0.596006\u001b[0m  |  \u001b[32m  0.556632\u001b[0m  |     1.070736  |     77.71%  |  9.4s\n",
      "    11  |  \u001b[94m  0.592442\u001b[0m  |  \u001b[32m  0.554197\u001b[0m  |     1.069010  |     78.07%  |  9.4s\n",
      "    12  |  \u001b[94m  0.585748\u001b[0m  |  \u001b[32m  0.552086\u001b[0m  |     1.060973  |     77.98%  |  9.4s\n",
      "    13  |  \u001b[94m  0.579392\u001b[0m  |  \u001b[32m  0.549509\u001b[0m  |     1.054382  |     78.04%  |  9.4s\n",
      "    14  |  \u001b[94m  0.574667\u001b[0m  |  \u001b[32m  0.548793\u001b[0m  |     1.047149  |     78.03%  |  9.3s\n",
      "    15  |  \u001b[94m  0.571650\u001b[0m  |  \u001b[32m  0.544742\u001b[0m  |     1.049396  |     78.06%  |  9.3s\n",
      "    16  |  \u001b[94m  0.565355\u001b[0m  |  \u001b[32m  0.540427\u001b[0m  |     1.046128  |     78.33%  |  9.3s\n",
      "    17  |  \u001b[94m  0.563660\u001b[0m  |  \u001b[32m  0.539958\u001b[0m  |     1.043896  |     78.55%  |  9.3s\n",
      "    18  |  \u001b[94m  0.561298\u001b[0m  |  \u001b[32m  0.537174\u001b[0m  |     1.044908  |     78.63%  |  9.3s\n",
      "    19  |  \u001b[94m  0.558390\u001b[0m  |    0.537198  |     1.039450  |     78.32%  |  9.3s\n",
      "    20  |    0.558703  |  \u001b[32m  0.535452\u001b[0m  |     1.043422  |     78.61%  |  9.2s\n",
      "    21  |  \u001b[94m  0.555898\u001b[0m  |  \u001b[32m  0.531518\u001b[0m  |     1.045867  |     78.79%  |  9.3s\n",
      "    22  |  \u001b[94m  0.550225\u001b[0m  |  \u001b[32m  0.530420\u001b[0m  |     1.037339  |     78.80%  |  9.3s\n",
      "    23  |  \u001b[94m  0.548070\u001b[0m  |    0.531777  |     1.030640  |     78.69%  |  9.1s\n",
      "    24  |  \u001b[94m  0.544148\u001b[0m  |  \u001b[32m  0.528902\u001b[0m  |     1.028824  |     78.81%  |  9.1s\n",
      "    25  |  \u001b[94m  0.542763\u001b[0m  |  \u001b[32m  0.525115\u001b[0m  |     1.033608  |     79.04%  |  9.1s\n",
      "    26  |  \u001b[94m  0.541371\u001b[0m  |  \u001b[32m  0.524368\u001b[0m  |     1.032425  |     78.88%  |  9.2s\n",
      "    27  |    0.542311  |    0.524997  |     1.032980  |     78.99%  |  9.1s\n",
      "    28  |  \u001b[94m  0.535353\u001b[0m  |  \u001b[32m  0.521036\u001b[0m  |     1.027478  |     79.02%  |  9.1s\n",
      "    29  |  \u001b[94m  0.534218\u001b[0m  |    0.521718  |     1.023960  |     78.95%  |  9.1s\n",
      "    30  |    0.534233  |    0.522403  |     1.022646  |     79.03%  |  9.1s\n",
      "    31  |  \u001b[94m  0.531416\u001b[0m  |  \u001b[32m  0.517954\u001b[0m  |     1.025989  |     79.22%  |  9.0s\n",
      "    32  |  \u001b[94m  0.530784\u001b[0m  |  \u001b[32m  0.517006\u001b[0m  |     1.026649  |     79.38%  |  9.1s\n",
      "    33  |  \u001b[94m  0.528174\u001b[0m  |    0.517345  |     1.020932  |     79.16%  |  9.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=300, dense1_num_units=300, dropout_p=0.65,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=100, more_params={}, on_epoch_finished=(),\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=0.01, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net6.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DropoutLayer      \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.860121\u001b[0m  |  \u001b[32m  0.647825\u001b[0m  |     1.327706  |     75.10%  |  10.9s\n",
      "     2  |  \u001b[94m  0.662327\u001b[0m  |  \u001b[32m  0.607538\u001b[0m  |     1.090182  |     76.43%  |  11.0s\n",
      "     3  |  \u001b[94m  0.627324\u001b[0m  |  \u001b[32m  0.587278\u001b[0m  |     1.068189  |     76.94%  |  11.0s\n",
      "     4  |  \u001b[94m  0.605131\u001b[0m  |  \u001b[32m  0.574362\u001b[0m  |     1.053571  |     77.47%  |  10.9s\n",
      "     5  |  \u001b[94m  0.588685\u001b[0m  |  \u001b[32m  0.564029\u001b[0m  |     1.043714  |     77.58%  |  10.9s\n",
      "     6  |  \u001b[94m  0.575891\u001b[0m  |  \u001b[32m  0.555900\u001b[0m  |     1.035962  |     77.84%  |  10.9s\n",
      "     7  |  \u001b[94m  0.565721\u001b[0m  |  \u001b[32m  0.549052\u001b[0m  |     1.030360  |     78.35%  |  11.2s\n",
      "     8  |  \u001b[94m  0.555439\u001b[0m  |  \u001b[32m  0.546447\u001b[0m  |     1.016455  |     78.34%  |  10.9s"
     ]
    }
   ],
   "source": [
    "net7.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to file my_neural_net_submission.csv.\n"
     ]
    }
   ],
   "source": [
    "make_submission(net0, X_test, ids, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
