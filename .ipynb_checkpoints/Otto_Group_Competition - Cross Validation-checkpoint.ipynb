{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Challenge using nolearn/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook is meant to help you getting started with nolearn and lasagne in order to train a neural net and make a submission to the Otto Group Product Classification Challenge.\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Get the notebook from the Otto Group repository](https://github.com/ottogroup)\n",
    "* [Nolearn repository](https://github.com/dnouri/nolearn)\n",
    "* [Lasagne repository](https://github.com/benanne/Lasagne)\n",
    "* [A nolearn/lasagne tutorial for convolutional nets](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum, sgd, adagrad\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sub = \"submissions/sampleSubmission.csv\"\n",
    "sample_sub_df = pd.read_csv(sample_sub)\n",
    "\n",
    "def normalize(row, epsilon=1e-15):\n",
    "    \n",
    "    row = row / np.sum(row)\n",
    "    row = np.maximum(epsilon, row)\n",
    "    row = np.minimum(1 - epsilon, row)\n",
    "    \n",
    "    return row\n",
    "    \n",
    "def logloss_mc(y_true, y_probs):\n",
    "    \n",
    "    # Normalize probability data frame\n",
    "    y_probs = y_probs.apply(normalize, axis=1)\n",
    "        \n",
    "    log_vals = []\n",
    "        \n",
    "    for i, y in enumerate(y_true):\n",
    "        c = int(y.split(\"_\")[1])\n",
    "        log_vals.append(- np.log(y_probs.iloc[i,c - 1]))\n",
    "        \n",
    "    return np.mean(log_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually split data and keep forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_man = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data_non_lasagne(df, train_size=0.8, percentage=1, standardize=False):\n",
    "\n",
    "    if standardize:\n",
    "        X = df.drop(['id', 'target'], axis=1).apply(func=log_normalize, axis=1)\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X = pd.DataFrame(X)\n",
    "        X.loc[:, 'id'] = df.loc[:, 'id']\n",
    "        X.loc[:, 'target'] = df.loc[:, 'target']\n",
    "        df = X\n",
    "    \n",
    "    num_samples = int(len(df) * percentage)\n",
    "    \n",
    "    sample_rows = random.sample(df.index, num_samples)\n",
    "    \n",
    "    df_sampled = df.ix[sample_rows]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df_sampled.drop(['id', 'target'], axis=1),\n",
    "                                                          df_sampled.target, \n",
    "                                                          train_size=train_size)\n",
    "    \n",
    "    return (X_train, X_valid,\n",
    "            y_train.astype(str), y_valid.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get random rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_train_data_non_lasagne(pd.read_csv(\"data/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data_cross_validation(X_train, y_train):\n",
    "    \n",
    "    X, labels = X_train.astype(np.float32), y_train\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data_cross_validation(X_valid, scaler):\n",
    "    X_valid, ids = X_valid.astype(np.float32), np.arange(1, len(y_valid) + 1).astype(str)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    return X_valid, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_data(path, scaler):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(X)\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, X_test, ids, encoder, name='my_neural_net_submission.csv'):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = [w.get_value() for w in nn.get_all_params()]\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_weights_from(self.best_weights)\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data_cross_validation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data_cross_validation(X_valid, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [('input', InputLayer),\n",
    "           ('dropout', DropoutLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout0', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('dropout1', DropoutLayer),\n",
    "           ('dense2', DenseLayer),\n",
    "           ('dropout2', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(layers=layers0,\n",
    "                 \n",
    "                 input_shape=(None, num_features),\n",
    "                 \n",
    "                 dropout_p=0.1,\n",
    "                 \n",
    "                 dense0_num_units=512,\n",
    "                 dropout0_p=0.2,\n",
    "                 \n",
    "                 dense1_num_units=1024,\n",
    "                 dropout1_p=0.3,\n",
    "                 \n",
    "                 dense2_num_units=512,\n",
    "                 dropout2_p=0.4,\n",
    "                 \n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,\n",
    "                 #update=sgd,\n",
    "                 # optimization method:\n",
    "                 update_learning_rate=theano.shared(float32(0.02)),\n",
    "                 update_momentum=theano.shared(float32(0.9)),\n",
    "\n",
    "\n",
    "                on_epoch_finished=[\n",
    "                    AdjustVariable('update_learning_rate', start=0.02, stop=0.001),\n",
    "                    AdjustVariable('update_momentum', start=0.9, stop=0.9999),\n",
    "                    EarlyStopping(patience=200),\n",
    "        ],\n",
    "                 \n",
    "                 eval_size=0.2,\n",
    "                 verbose=1,\n",
    "                 max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to file cross_validation1.csv.\n"
     ]
    }
   ],
   "source": [
    "make_submission(net2, X_test, ids, encoder, \"cross_validation1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probs = pd.read_csv(\"cross_validation1.csv\").iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52981008250282702"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_mc(y_valid, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polish submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polish(row, threshold=0.01):\n",
    "    \n",
    "    for i, x in enumerate(row):\n",
    "        if x < threshold:\n",
    "            row[i] = 0\n",
    "            \n",
    "    return row    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_probs = y_probs.apply(polish, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /home/pold/Documents/Radboud/otto-find-ich-gut/submissions/sampleSubmission.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.14 secs.\n",
      "------------------------------------------------------PROGRESS:     18   8.679e-01      455.24s\n",
      "PROGRESS:     19   8.698e-01      466.31s\n",
      "PROGRESS:     20   8.720e-01      502.06s\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "graph_train = pd.DataFrame(X, columns=df_train.columns[1:-1])\n",
    "graph_train['target'] = y\n",
    "\n",
    "train = gl.SFrame(graph_train)\n",
    "\n",
    "graph_test = pd.DataFrame(X_test, columns=df_test.columns[1:])\n",
    "\n",
    "test = gl.SFrame(graph_test)\n",
    "sample = gl.SFrame.read_csv('submissions/sampleSubmission.csv')\n",
    "\n",
    "# Train a model\n",
    "m = gl.boosted_trees_classifier.create(dataset = train,\n",
    "                                       target='target',\n",
    "                                       max_iterations=100,\n",
    "                                       max_depth = 10,\n",
    "                                       row_subsample = 0.86,\n",
    "                                       column_subsample = 0.78,\n",
    "                                       min_loss_reduction = 1.05,\n",
    "                                       min_child_weight = 5,\n",
    "                                       validation_set = None)\n",
    " \n",
    "# Make submission\n",
    "preds = m.predict_topk(test, output_type='probability', k=9)\n",
    "preds['id'] = preds['id'].astype(int) + 1\n",
    "preds = preds.unstack(['class', 'probability'], 'probs').unpack('probs', '')\n",
    "preds = preds.sort('id')\n",
    "\n",
    "preds.save(\"graphlab_crazy_cross.csv\", format = 'csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crazy_graphlab = pd.read_csv(\"graphlab_crazy_cross.csv\").iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54265855159833221"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_mc(y_valid, crazy_graphlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "absolute_crazy = pd.DataFrame(0.1 * crazy_graphlab.as_matrix() + 0.9 * y_probs.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p is:  0.0\n",
      "0.529810082503\n",
      "\n",
      "p is:  0.1\n",
      "0.51785465833\n",
      "\n",
      "p is:  0.2\n",
      "0.513138476566\n",
      "\n",
      "p is:  0.3\n",
      "0.510888234464\n",
      "\n",
      "p is:  0.4\n",
      "0.510467995433\n",
      "\n",
      "p is:  0.5\n",
      "0.511619023379\n",
      "\n",
      "p is:  0.6\n",
      "0.514251064076\n",
      "\n",
      "p is:  0.7\n",
      "0.518390332656\n",
      "\n",
      "p is:  0.8\n",
      "0.524185774811\n",
      "\n",
      "p is:  0.9\n",
      "0.531982907178\n",
      "\n",
      "p is:  1.0\n",
      "0.542658551598\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in np.linspace(0,1,11):\n",
    "    print \"p is: \", p\n",
    "    combined = pd.DataFrame(p * crazy_graphlab.as_matrix() +\n",
    "                            (1 - p) * y_probs.as_matrix())\n",
    "    print logloss_mc(y_valid, combined)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51785465833004651"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_mc(y_valid, absolute_crazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00879195,  1.20103565,  0.6545801 , ...,  0.01923468,\n",
       "         0.04363248,  0.0081767 ],\n",
       "       [ 0.02719556,  0.00502506,  0.00359961, ...,  0.02017262,\n",
       "         0.33962073,  0.0124155 ],\n",
       "       [ 0.13292915,  0.01942159,  0.00791098, ...,  0.0147196 ,\n",
       "         1.65915004,  0.05366536],\n",
       "       ..., \n",
       "       [ 0.10482673,  1.33743742,  0.11034114, ...,  0.08099743,\n",
       "         0.06426095,  0.13559176],\n",
       "       [ 0.14020137,  0.12392503,  0.09462741, ...,  0.13060482,\n",
       "         0.32985689,  0.30654467],\n",
       "       [ 0.4304514 ,  0.02294892,  0.02172343, ...,  0.20669216,\n",
       "         0.2282868 ,  0.07542733]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_crazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
