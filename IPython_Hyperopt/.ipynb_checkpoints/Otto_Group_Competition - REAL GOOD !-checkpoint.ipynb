{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Challenge using nolearn/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook is meant to help you getting started with nolearn and lasagne in order to train a neural net and make a submission to the Otto Group Product Classification Challenge.\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Get the notebook from the Otto Group repository](https://github.com/ottogroup)\n",
    "* [Nolearn repository](https://github.com/dnouri/nolearn)\n",
    "* [Lasagne repository](https://github.com/benanne/Lasagne)\n",
    "* [A nolearn/lasagne tutorial for convolutional nets](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 210\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax, leaky_rectify, LeakyRectify\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_data(path, scaler):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.iloc[:,:-1]\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(X)\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, X_test, ids, encoder):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    i = 0\n",
    "    while os.path.exists(os.path.join(\"submissions\", \"nn-\" + str(i) + \".csv\")):\n",
    "        i += 1\n",
    "    name = os.path.join(\"submissions\", \"nn-\" + str(i) + \".csv\")\n",
    "    \n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data(\"../data/train80.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data('../data/holdout20.csv', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust network parameters over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = [w.get_value() for w in nn.get_all_params()]\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_weights_from(self.best_weights)\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [('input', InputLayer),\n",
    "           ('dropoutin', DropoutLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout0', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('dropout1', DropoutLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_sub = \"../submissions/sampleSubmission.csv\"\n",
    "sample_sub_df = pd.read_csv(sample_sub)\n",
    "\n",
    "def normalize(row, epsilon=1e-15):\n",
    "    \n",
    "    row = row / np.sum(row)\n",
    "    row = np.maximum(epsilon, row)\n",
    "    row = np.minimum(1 - epsilon, row)\n",
    "    \n",
    "    return row\n",
    "    \n",
    "def logloss_mc(y_true, y_probs):\n",
    "    \n",
    "    # Normalize probability data frame\n",
    "    y_probs = y_probs.apply(normalize, axis=1)\n",
    "        \n",
    "    log_vals = []\n",
    "        \n",
    "    for i, y in enumerate(y_true):\n",
    "        c = int(y.split(\"_\")[1])\n",
    "        log_vals.append(- np.log(y_probs.iloc[i,c - 1]))\n",
    "        \n",
    "    return np.mean(log_vals)\n",
    "\n",
    "df_holdout = pd.read_csv(\"../data/holdout20.csv\")\n",
    "y_valid = df_holdout.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import functools\n",
    "\n",
    "\n",
    "def f21(f):\n",
    "    return theano.shared(float32(f))\n",
    "\n",
    "space = {  'layers' : layers0,\n",
    "                'input_shape' : (None, num_features),\n",
    "                \n",
    "                'dropoutin_p' : hp.uniform('dropin', 0, 0.2),\n",
    "                \n",
    "                'dense0_num_units': hp.quniform('dense0', 200, 600, 30),\n",
    "                'dense0_nonlinearity' : hp.choice('leaky0', [LeakyRectify(x) for x in np.linspace(0, 1, 6)]) ,\n",
    "                'dropout0_p': hp.uniform('drop0', 0, 0.5),\n",
    "                \n",
    "                'dense1_num_units' : hp.quniform('dense1', 200, 600, 30),\n",
    "                'dense1_nonlinearity' : hp.choice('leaky1', [LeakyRectify(x) for x in np.linspace(0, 1, 6)]),\n",
    "                'dropout1_p' : hp.uniform('drop1', 0, 0.6),\n",
    "                \n",
    "                'output_num_units' : num_classes,\n",
    "                'output_nonlinearity' : softmax,\n",
    "\n",
    "                'update' : nesterov_momentum,\n",
    "              \n",
    "                'update_learning_rate' : hp.choice('ulr', [f21(f) for f in np.linspace(0.01, 0.03, 10)]),\n",
    "                'update_momentum' : hp.choice('um', [f21(f) for f in np.linspace(0.9, 0.99, 5)]),\n",
    "                'eval_size' : None,\n",
    "                'verbose' : 1,\n",
    "                'max_epochs' : 70}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different submission method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission_hyper(clf, X_test, ids, encoder, name='../hypersub.csv'):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR (theano.gof.opt): Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR:theano.gof.opt:Optimization failure due to: local_gpu_softmax_with_bias\n",
      "ERROR (theano.gof.opt): TRACEBACK:\n",
      "ERROR:theano.gof.opt:TRACEBACK:\n",
      "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n",
      "ERROR:theano.gof.opt:Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/opt.py\", line 1491, in process_node\n",
      "    replacements = lopt.transform(node)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/opt.py\", line 1119, in local_gpu_softmax_with_bias\n",
      "    gpu_sm = GpuSoftmaxWithBias()(gpu_from_host(x), gpu_from_host(b))\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/gof/op.py\", line 488, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.py\", line 133, in make_node\n",
      "    dtype=x.dtype)()])\n",
      "  File \"/usr/lib/python2.7/site-packages/theano/sandbox/cuda/type.py\", line 69, in __init__\n",
      "    (self.__class__.__name__, dtype, name))\n",
      "TypeError: CudaNdarrayType only supports dtype float32 for now. Tried using dtype float64 for variable None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DropoutLayer      \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 510.0)       \tproduces   510.0 outputs\n",
      "  DropoutLayer      \t(None, 510.0)       \tproduces   510.0 outputs\n",
      "  DenseLayer        \t(None, 360.0)       \tproduces   360.0 outputs\n",
      "  DropoutLayer      \t(None, 360.0)       \tproduces   360.0 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.838694\u001b[0m  |         nan  |          nan  |       nan%  |  29.2s\n",
      "     2  |  \u001b[94m  0.761991\u001b[0m  |         nan  |          nan  |       nan%  |  29.7s\n",
      "     3  |  \u001b[94m  0.749804\u001b[0m  |         nan  |          nan  |       nan%  |  29.1s\n",
      "     4  |  \u001b[94m  0.746817\u001b[0m  |         nan  |          nan  |       nan%  |  28.9s"
     ]
    }
   ],
   "source": [
    "# XGBoost polished\n",
    "subpol = pd.read_csv(\"../submission.csv\").iloc[:, 1:]\n",
    "logloss_mc(y_valid, subpol)\n",
    "\n",
    "\n",
    "def objective(hyperparameter):\n",
    "\n",
    "    mynet = NeuralNet(** hyperparameter)\n",
    "    mynet.fit(X, y)\n",
    "\n",
    "    make_submission_hyper(mynet, X_test, ids, encoder)\n",
    "\n",
    "    sub = pd.read_csv(\"../hypersub.csv\").iloc[:, 1:]\n",
    "    ll = logloss_mc(y_valid, sub)\n",
    "    print(ll)\n",
    "    print(hyperparameter)\n",
    "    return {'loss' : ll,\n",
    "            'status' : STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials)\n",
    "\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print trials.trials()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
