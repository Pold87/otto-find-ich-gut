{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otto Group Product Classification Challenge using nolearn/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook is meant to help you getting started with nolearn and lasagne in order to train a neural net and make a submission to the Otto Group Product Classification Challenge.\n",
    "\n",
    "* [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)\n",
    "* [Get the notebook from the Otto Group repository](https://github.com/ottogroup)\n",
    "* [Nolearn repository](https://github.com/dnouri/nolearn)\n",
    "* [Lasagne repository](https://github.com/benanne/Lasagne)\n",
    "* [A nolearn/lasagne tutorial for convolutional nets](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_sub = \"submissions/sampleSubmission.csv\"\n",
    "sample_sub_df = pd.read_csv(sample_sub)\n",
    "\n",
    "def normalize(row, epsilon=1e-15):\n",
    "    \n",
    "    row = row / np.sum(row)\n",
    "    row = np.maximum(epsilon, row)\n",
    "    row = np.minimum(1 - epsilon, row)\n",
    "    \n",
    "    return row\n",
    "    \n",
    "def logloss_mc(y_true, y_probs):\n",
    "    \n",
    "    # Normalize probability data frame\n",
    "    y_probs = y_probs.apply(normalize, axis=1)\n",
    "        \n",
    "    log_vals = []\n",
    "        \n",
    "    for i, y in enumerate(y_true):\n",
    "        c = int(y.split(\"_\")[1])\n",
    "        log_vals.append(- np.log(y_probs.iloc[i,c - 1]))\n",
    "        \n",
    "    return np.mean(log_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data_non_lasagne(df, train_size=0.8, percentage=1, standardize=False):\n",
    "\n",
    "    if standardize:\n",
    "        X = df.drop(['id', 'target'], axis=1).apply(func=log_normalize, axis=1)\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X = pd.DataFrame(X)\n",
    "        X.loc[:, 'id'] = df.loc[:, 'id']\n",
    "        X.loc[:, 'target'] = df.loc[:, 'target']\n",
    "        df = X\n",
    "\n",
    "    \n",
    "    \n",
    "    num_samples = int(len(df) * percentage)\n",
    "    \n",
    "    sample_rows = random.sample(df.index, num_samples)\n",
    "    \n",
    "    df_sampled = df.ix[sample_rows]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(df_sampled.drop(['id', 'target'], axis=1),\n",
    "                                                          df_sampled.target, \n",
    "                                                          train_size=train_size)\n",
    "    \n",
    "    return (X_train, X_valid,\n",
    "            y_train.astype(str), y_valid.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get random rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_train_data_non_lasagne(pd.read_csv(\"data/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12376"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data_cross_validation(X_train, y_train):\n",
    "    \n",
    "    X, labels = X_train.astype(np.float32), y_train\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_data_cross_validation(X_valid, scaler):\n",
    "    X_valid, ids = X_valid.astype(np.float32), np.arange(1, len(y_valid) + 1).astype(str)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    return X_valid, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    np.random.shuffle(X)\n",
    "    X, labels = X[:, 1:-1].astype(np.float32), X[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(labels).astype(np.int32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_test_data(path, scaler):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.values.copy()\n",
    "    X, ids = X[:, 1:].astype(np.float32), X[:, 0].astype(str)\n",
    "    X = scaler.transform(X)\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, X_test, ids, encoder, name='my_neural_net_submission.csv'):\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    with open(name, 'w') as f:\n",
    "        f.write('id,')\n",
    "        f.write(','.join(encoder.classes_))\n",
    "        f.write('\\n')\n",
    "        for id, probs in zip(ids, y_prob):\n",
    "            probas = ','.join([id] + map(str, probs.tolist()))\n",
    "            f.write(probas)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote submission to file {}.\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=100):\n",
    "        self.patience = patience\n",
    "        self.best_valid = np.inf\n",
    "        self.best_valid_epoch = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        current_valid = train_history[-1]['valid_loss']\n",
    "        current_epoch = train_history[-1]['epoch']\n",
    "        if current_valid < self.best_valid:\n",
    "            self.best_valid = current_valid\n",
    "            self.best_valid_epoch = current_epoch\n",
    "            self.best_weights = [w.get_value() for w in nn.get_all_params()]\n",
    "        elif self.best_valid_epoch + self.patience < current_epoch:\n",
    "            print(\"Early stopping.\")\n",
    "            print(\"Best valid loss was {:.6f} at epoch {}.\".format(\n",
    "                self.best_valid, self.best_valid_epoch))\n",
    "            nn.load_weights_from(self.best_weights)\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data('data/test.csv', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)\n",
    "num_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, encoder, scaler = load_train_data_cross_validation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, ids = load_test_data_cross_validation(X_valid, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers0 = [('input', InputLayer),\n",
    "           ('dense0', DenseLayer),\n",
    "           ('dropout', DropoutLayer),\n",
    "           ('dense1', DenseLayer),\n",
    "           ('output', DenseLayer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(layers=layers0,\n",
    "                 \n",
    "                 input_shape=(None, num_features),\n",
    "                 dense0_num_units=300,\n",
    "                 dropout_p=0.5,\n",
    "                 dense1_num_units=300,\n",
    "                 output_num_units=num_classes,\n",
    "                 output_nonlinearity=softmax,\n",
    "                 \n",
    "                 update=nesterov_momentum,\n",
    "                 \n",
    "                 # optimization method:\n",
    "                 update_learning_rate=theano.shared(float32(0.05)),\n",
    "                 update_momentum=theano.shared(float32(0.8)),\n",
    "\n",
    "\n",
    "                on_epoch_finished=[\n",
    "                    AdjustVariable('update_learning_rate', start=0.05, stop=0.00001),\n",
    "                    AdjustVariable('update_momentum', start=0.8, stop=0.9999),\n",
    "                    EarlyStopping(patience=200),\n",
    "        ],\n",
    "                 \n",
    "                 eval_size=0.2,\n",
    "                 verbose=1,\n",
    "                 max_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InputLayer        \t(None, 93)          \tproduces      93 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DropoutLayer      \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 300)         \tproduces     300 outputs\n",
      "  DenseLayer        \t(None, 9)           \tproduces       9 outputs\n",
      "\n",
      " Epoch  |  Train loss  |  Valid loss  |  Train / Val  |  Valid acc  |  Dur\n",
      "--------|--------------|--------------|---------------|-------------|-------\n",
      "     1  |  \u001b[94m  0.818763\u001b[0m  |  \u001b[32m  0.643108\u001b[0m  |     1.273134  |     74.71%  |  8.6s\n",
      "     2  |  \u001b[94m  0.658205\u001b[0m  |  \u001b[32m  0.598454\u001b[0m  |     1.099842  |     76.88%  |  8.8s\n",
      "     3  |  \u001b[94m  0.622278\u001b[0m  |  \u001b[32m  0.582854\u001b[0m  |     1.067640  |     77.42%  |  8.5s\n",
      "     4  |  \u001b[94m  0.598871\u001b[0m  |  \u001b[32m  0.570920\u001b[0m  |     1.048958  |     77.60%  |  8.5s\n",
      "     5  |  \u001b[94m  0.587366\u001b[0m  |  \u001b[32m  0.557764\u001b[0m  |     1.053072  |     78.21%  |  8.3s\n",
      "     6  |  \u001b[94m  0.575259\u001b[0m  |  \u001b[32m  0.553131\u001b[0m  |     1.040005  |     78.53%  |  8.3s\n",
      "     7  |  \u001b[94m  0.562774\u001b[0m  |  \u001b[32m  0.543344\u001b[0m  |     1.035760  |     78.80%  |  8.2s\n",
      "     8  |  \u001b[94m  0.554130\u001b[0m  |  \u001b[32m  0.542921\u001b[0m  |     1.020645  |     78.88%  |  8.5s\n",
      "     9  |  \u001b[94m  0.546789\u001b[0m  |  \u001b[32m  0.537063\u001b[0m  |     1.018109  |     79.05%  |  8.2s\n",
      "    10  |  \u001b[94m  0.540974\u001b[0m  |    0.540194  |     1.001444  |     78.87%  |  8.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=<function matrix at 0x7fc9ed37f140>,\n",
       "     batch_iterator_test=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3cd10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.BatchIterator object at 0x7fc9e9f3ccd0>,\n",
       "     dense0_num_units=300, dense1_num_units=300, dropout_p=0.5,\n",
       "     eval_size=0.2, input_shape=(None, 93),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=<function negative_log_likelihood at 0x7fc9ea861668>,\n",
       "     max_epochs=10, more_params={},\n",
       "     on_epoch_finished=[<__main__.AdjustVariable object at 0x7fc9dd7a4c10>, <__main__.AdjustVariable object at 0x7fc9dd7e34d0>, <__main__.EarlyStopping object at 0x7fc9dd7e3750>],\n",
       "     on_training_finished=(),\n",
       "     output_nonlinearity=<theano.tensor.nnet.nnet.Softmax object at 0x7fc9ecfcab90>,\n",
       "     output_num_units=9, regression=False,\n",
       "     update=<function nesterov_momentum at 0x7fc9ea861320>,\n",
       "     update_learning_rate=<TensorType(float32, scalar)>,\n",
       "     update_momentum=<TensorType(float32, scalar)>,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote submission to file cross_validation1.csv.\n"
     ]
    }
   ],
   "source": [
    "make_submission(net2, X_test, ids, encoder, \"cross_validation1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probs = pd.read_csv(\"cross_validation1.csv\").iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70197506680714838"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss_mc(y_true, y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polish submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polish(row, threshold=0.01):\n",
    "    \n",
    "    for i, x in enumerate(row):\n",
    "        if x < threshold:\n",
    "            row[i] = 0\n",
    "            \n",
    "    return row    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_probs = y_probs.apply(polish, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.692817e-03</td>\n",
       "      <td>1.918713e-14</td>\n",
       "      <td>8.015072e-15</td>\n",
       "      <td>1.195318e-12</td>\n",
       "      <td>1.257551e-13</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1.438510e-07</td>\n",
       "      <td>2.203745e-09</td>\n",
       "      <td>8.658806e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.462026e-06</td>\n",
       "      <td>6.189851e-01</td>\n",
       "      <td>2.689075e-03</td>\n",
       "      <td>1.289277e-04</td>\n",
       "      <td>5.388960e-06</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>6.305716e-05</td>\n",
       "      <td>7.709063e-06</td>\n",
       "      <td>8.720888e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.631211e-06</td>\n",
       "      <td>4.855975e-01</td>\n",
       "      <td>6.016642e-02</td>\n",
       "      <td>7.889919e-04</td>\n",
       "      <td>8.745908e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.344715e-05</td>\n",
       "      <td>3.622999e-06</td>\n",
       "      <td>4.265114e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.222972e-14</td>\n",
       "      <td>1.089753e-14</td>\n",
       "      <td>6.575821e-15</td>\n",
       "      <td>2.454846e-14</td>\n",
       "      <td>3.360689e-18</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>1.431133e-12</td>\n",
       "      <td>9.558473e-09</td>\n",
       "      <td>6.313265e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.059270e-05</td>\n",
       "      <td>2.503575e-13</td>\n",
       "      <td>4.841479e-14</td>\n",
       "      <td>4.741045e-13</td>\n",
       "      <td>1.498830e-12</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3.547771e-11</td>\n",
       "      <td>8.205999e-08</td>\n",
       "      <td>9.814084e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class_1       Class_2       Class_3       Class_4       Class_5  \\\n",
       "0  3.692817e-03  1.918713e-14  8.015072e-15  1.195318e-12  1.257551e-13   \n",
       "1  7.462026e-06  6.189851e-01  2.689075e-03  1.289277e-04  5.388960e-06   \n",
       "2  4.631211e-06  4.855975e-01  6.016642e-02  7.889919e-04  8.745908e-07   \n",
       "3  1.222972e-14  1.089753e-14  6.575821e-15  2.454846e-14  3.360689e-18   \n",
       "4  2.059270e-05  2.503575e-13  4.841479e-14  4.741045e-13  1.498830e-12   \n",
       "\n",
       "    Class_6       Class_7       Class_8       Class_9  \n",
       "0  0.000069  1.438510e-07  2.203745e-09  8.658806e-01  \n",
       "1  0.017242  6.305716e-05  7.709063e-06  8.720888e-06  \n",
       "2  0.000000  1.344715e-05  3.622999e-06  4.265114e-04  \n",
       "3  0.999796  1.431133e-12  9.558473e-09  6.313265e-12  \n",
       "4  0.000020  3.547771e-11  8.205999e-08  9.814084e-01  "
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probs = y_probs * y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Exception trying to retrieve Hadoop classpath: [Errno 12] Cannot allocate memory\n",
      "[ERROR] Fatal error. The unity_server process cannot be started. There may have been an issue during installation of graphlab-create. Please uninstall graphlab-create and reinstall it, looking for errors that may occur during installation. If the problem persists, please contact support@dato.com.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot connect to GraphLab Server",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-361-b3ce518ad8c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgraph_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submissions/sampleSubmission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/data_structures/sframe.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, format, _proxy)\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__proxy__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_proxy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__proxy__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnitySFrameProxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglconnect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_client\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m             \u001b[0m_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/graphlab/connect/main.pyc\u001b[0m in \u001b[0;36mget_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mis_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot connect to GraphLab Server\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__CLIENT__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Cannot connect to GraphLab Server"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "graph_df = pd.DataFrame(X)\n",
    "graph_df['target'] = y\n",
    "\n",
    "train = gl.SFrame(graph_df)\n",
    "test = gl.SFrame(X_test)\n",
    "sample = gl.SFrame.read_csv('submissions/sampleSubmission.csv')\n",
    "\n",
    "del train['id']\n",
    "\n",
    "# Train a model\n",
    "m = gl.boosted_trees_classifier.create(dataset = train,\n",
    "                                       target='target',\n",
    "                                       max_iterations=100,\n",
    "                                       max_depth = 10,\n",
    "                                       row_subsample = 0.86,\n",
    "                                       column_subsample = 0.78,\n",
    "                                       min_loss_reduction = 1.05,\n",
    "                                       min_child_weight = 4,\n",
    "                                       validation_set = None)\n",
    " \n",
    "# Make submission\n",
    "preds = m.predict_topk(test, output_type='probability', k=9)\n",
    "preds['id'] = preds['id'].astype(int) + 1\n",
    "preds = preds.unstack(['class', 'probability'], 'probs').unpack('probs', '')\n",
    "preds = preds.sort('id')\n",
    " \n",
    "assert sample.num_rows() == preds.num_rows()\n",
    "\n",
    "preds.save(\"graphlab_crazy_cross.csv\", format = 'csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
